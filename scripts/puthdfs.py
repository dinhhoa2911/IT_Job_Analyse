from hdfs import InsecureClient
import pandas as pd
import os

# ======================================================
# CONFIGURATION
# ======================================================
HDFS_HOST = "dinhhoa-master"
HDFS_PORT = 9000
HDFS_USER = "root"

LOCAL_FILE = "/opt/spark/scripts/itviec_jobs_full.csv"
HDFS_PATH = "/data_lake/raw/itviec_jobs_full.csv"
TMP_HDFS_LOCAL = "/opt/spark/scripts/hdfs_old.csv"

print("üöÄ B·∫Øt ƒë·∫ßu c·∫≠p nh·∫≠t file ITviec tr√™n HDFS (ch·ªâ th√™m record m·ªõi)...")

try:
    hdfs_url = f"http://{HDFS_HOST}:{HDFS_PORT}"
    client = InsecureClient(hdfs_url, user=HDFS_USER)

    # ======================================================
    #  T·∫£i file HDFS hi·ªán t·∫°i (n·∫øu c√≥)
    # ======================================================
    if client.status(HDFS_PATH, strict=False):
        print(f"üì• ƒêang t·∫£i file c≈© t·ª´ HDFS: {HDFS_PATH}")
        with client.read(HDFS_PATH, encoding="utf-8") as reader:
            old_df = pd.read_csv(reader)
        print(f"üìÑ File c≈© c√≥ {len(old_df)} d√≤ng.")
    else:
        print("üÜï Kh√¥ng c√≥ file c≈© tr√™n HDFS.")
        old_df = pd.DataFrame()

    # ======================================================
    #  ƒê·ªçc file m·ªõi local
    # ======================================================
    if not os.path.exists(LOCAL_FILE):
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file local: {LOCAL_FILE}")
    new_df = pd.read_csv(LOCAL_FILE)
    print(f"üÜï File m·ªõi c√≥ {len(new_df)} d√≤ng.")

    # ======================================================
    #  G·ªôp + lo·∫°i tr√πng theo job_link
    # ======================================================
    if not old_df.empty:
        combined_df = pd.concat([old_df, new_df], ignore_index=True)
        combined_df.drop_duplicates(subset=["job_link"], inplace=True)
        print(f"üìä Sau khi g·ªôp v√† lo·∫°i tr√πng: {len(combined_df)} d√≤ng.")
    else:
        combined_df = new_df

    # ======================================================
    #  L∆∞u t·∫°m ra local r·ªìi upload ng∆∞·ª£c l√™n HDFS
    # ======================================================
    combined_df.to_csv(TMP_HDFS_LOCAL, index=False, encoding="utf-8-sig")

    client.makedirs(os.path.dirname(HDFS_PATH))
    client.upload(HDFS_PATH, TMP_HDFS_LOCAL, overwrite=True)

    print(f" ƒê√£ c·∫≠p nh·∫≠t th√†nh c√¥ng: {HDFS_PATH}")
    print(f" T·ªïng s·ªë record hi·ªán t·∫°i: {len(combined_df)}")
except Exception as e:
    print(f" L·ªói c·∫≠p nh·∫≠t HDFS: {e}")
