[2025-10-31T14:53:40.510+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-10-31T14:53:40.549+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_without_crawl.load_bronze manual__2025-10-31T14:53:32.974462+00:00 [queued]>
[2025-10-31T14:53:40.559+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_without_crawl.load_bronze manual__2025-10-31T14:53:32.974462+00:00 [queued]>
[2025-10-31T14:53:40.559+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-10-31T14:53:40.572+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): load_bronze> on 2025-10-31 14:53:32.974462+00:00
[2025-10-31T14:53:40.577+0000] {standard_task_runner.py:72} INFO - Started process 203 to run task
[2025-10-31T14:53:40.581+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'etl_without_crawl', 'load_bronze', 'manual__2025-10-31T14:53:32.974462+00:00', '--job-id', '76', '--raw', '--subdir', 'DAGS_FOLDER/ETL_Without_Crawl.py', '--cfg-path', '/tmp/tmp7k2q947s']
[2025-10-31T14:53:40.583+0000] {standard_task_runner.py:105} INFO - Job 76: Subtask load_bronze
[2025-10-31T14:53:40.608+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.8/site-packages/***/settings.py:209: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  SQL_ALCHEMY_CONN = conf.get("database", "SQL_ALCHEMY_CONN")

[2025-10-31T14:53:40.688+0000] {task_command.py:467} INFO - Running <TaskInstance: etl_without_crawl.load_bronze manual__2025-10-31T14:53:32.974462+00:00 [running]> on host a1de7f96d4f1
[2025-10-31T14:53:29.356+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='alert@datateam.local' AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='etl_without_crawl' AIRFLOW_CTX_TASK_ID='load_bronze' AIRFLOW_CTX_EXECUTION_DATE='2025-10-31T14:53:32.974462+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-10-31T14:53:32.974462+00:00'
[2025-10-31T14:53:29.357+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-10-31T14:53:29.381+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-10-31T14:53:29.383+0000] {subprocess.py:88} INFO - Running command: ['/bin/bash', '-c', "\n        docker exec spark-submit bash -c '\n        /opt/spark/bin/spark-submit             --master spark://spark-master:7077             --deploy-mode client             --name Load_Bronze_Hive_Iceberg             --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2             /opt/spark/scripts/Load_Bronze.py\n        '\n        "]
[2025-10-31T14:53:29.400+0000] {subprocess.py:99} INFO - Output:
[2025-10-31T14:53:44.098+0000] {subprocess.py:106} INFO - :: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-10-31T14:53:44.251+0000] {subprocess.py:106} INFO - Ivy Default Cache set to: /home/spark/.ivy2/cache
[2025-10-31T14:53:44.252+0000] {subprocess.py:106} INFO - The jars for the packages stored in: /home/spark/.ivy2/jars
[2025-10-31T14:53:44.267+0000] {subprocess.py:106} INFO - org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency
[2025-10-31T14:53:44.276+0000] {subprocess.py:106} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-002f2f7e-b670-4541-a37e-6d523137b587;1.0
[2025-10-31T14:53:44.283+0000] {subprocess.py:106} INFO - 	confs: [default]
[2025-10-31T14:53:44.421+0000] {subprocess.py:106} INFO - 	found org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 in central
[2025-10-31T14:53:44.458+0000] {subprocess.py:106} INFO - :: resolution report :: resolve 183ms :: artifacts dl 7ms
[2025-10-31T14:53:44.459+0000] {subprocess.py:106} INFO - 	:: modules in use:
[2025-10-31T14:53:44.459+0000] {subprocess.py:106} INFO - 	org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 from central in [default]
[2025-10-31T14:53:44.460+0000] {subprocess.py:106} INFO - 	---------------------------------------------------------------------
[2025-10-31T14:53:44.461+0000] {subprocess.py:106} INFO - 	|                  |            modules            ||   artifacts   |
[2025-10-31T14:53:44.461+0000] {subprocess.py:106} INFO - 	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-10-31T14:53:44.462+0000] {subprocess.py:106} INFO - 	---------------------------------------------------------------------
[2025-10-31T14:53:44.463+0000] {subprocess.py:106} INFO - 	|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |
[2025-10-31T14:53:44.463+0000] {subprocess.py:106} INFO - 	---------------------------------------------------------------------
[2025-10-31T14:53:44.464+0000] {subprocess.py:106} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-002f2f7e-b670-4541-a37e-6d523137b587
[2025-10-31T14:53:44.464+0000] {subprocess.py:106} INFO - 	confs: [default]
[2025-10-31T14:53:44.470+0000] {subprocess.py:106} INFO - 	0 artifacts copied, 1 already retrieved (0kB/8ms)
[2025-10-31T14:53:44.732+0000] {subprocess.py:106} INFO - 25/10/31 14:53:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-10-31T14:53:45.928+0000] {subprocess.py:106} INFO - 25/10/31 14:53:45 INFO SparkContext: Running Spark version 3.5.3
[2025-10-31T14:53:45.928+0000] {subprocess.py:106} INFO - 25/10/31 14:53:45 INFO SparkContext: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64
[2025-10-31T14:53:45.929+0000] {subprocess.py:106} INFO - 25/10/31 14:53:45 INFO SparkContext: Java version 11.0.24
[2025-10-31T14:53:45.963+0000] {subprocess.py:106} INFO - 25/10/31 14:53:45 INFO ResourceUtils: ==============================================================
[2025-10-31T14:53:45.964+0000] {subprocess.py:106} INFO - 25/10/31 14:53:45 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-10-31T14:53:45.965+0000] {subprocess.py:106} INFO - 25/10/31 14:53:45 INFO ResourceUtils: ==============================================================
[2025-10-31T14:53:45.966+0000] {subprocess.py:106} INFO - 25/10/31 14:53:45 INFO SparkContext: Submitted application: Load_Bronze_Hive_Iceberg
[2025-10-31T14:53:46.000+0000] {subprocess.py:106} INFO - 25/10/31 14:53:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-10-31T14:53:46.016+0000] {subprocess.py:106} INFO - 25/10/31 14:53:46 INFO ResourceProfile: Limiting resource is cpu
[2025-10-31T14:53:46.017+0000] {subprocess.py:106} INFO - 25/10/31 14:53:46 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-10-31T14:53:46.111+0000] {subprocess.py:106} INFO - 25/10/31 14:53:46 INFO SecurityManager: Changing view acls to: spark
[2025-10-31T14:53:46.113+0000] {subprocess.py:106} INFO - 25/10/31 14:53:46 INFO SecurityManager: Changing modify acls to: spark
[2025-10-31T14:53:46.116+0000] {subprocess.py:106} INFO - 25/10/31 14:53:46 INFO SecurityManager: Changing view acls groups to:
[2025-10-31T14:53:46.118+0000] {subprocess.py:106} INFO - 25/10/31 14:53:46 INFO SecurityManager: Changing modify acls groups to:
[2025-10-31T14:53:46.119+0000] {subprocess.py:106} INFO - 25/10/31 14:53:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
[2025-10-31T14:53:46.415+0000] {subprocess.py:106} INFO - 25/10/31 14:53:46 INFO Utils: Successfully started service 'sparkDriver' on port 44183.
[2025-10-31T14:53:46.461+0000] {subprocess.py:106} INFO - 25/10/31 14:53:46 INFO SparkEnv: Registering MapOutputTracker
[2025-10-31T14:53:46.508+0000] {subprocess.py:106} INFO - 25/10/31 14:53:46 INFO SparkEnv: Registering BlockManagerMaster
[2025-10-31T14:53:46.532+0000] {subprocess.py:106} INFO - 25/10/31 14:53:46 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-10-31T14:53:46.533+0000] {subprocess.py:106} INFO - 25/10/31 14:53:46 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-10-31T14:53:46.538+0000] {subprocess.py:106} INFO - 25/10/31 14:53:46 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-10-31T14:53:46.572+0000] {subprocess.py:106} INFO - 25/10/31 14:53:46 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4d54db95-ce0a-4fed-b4b5-6510f70cc759
[2025-10-31T14:53:46.589+0000] {subprocess.py:106} INFO - 25/10/31 14:53:46 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-10-31T14:53:46.609+0000] {subprocess.py:106} INFO - 25/10/31 14:53:46 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-10-31T14:53:46.770+0000] {subprocess.py:106} INFO - 25/10/31 14:53:46 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-10-31T14:53:46.826+0000] {subprocess.py:106} INFO - 25/10/31 14:53:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-10-31T14:53:46.868+0000] {subprocess.py:106} INFO - 25/10/31 14:53:46 INFO SparkContext: Added JAR file:///home/spark/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar at spark://68d0d8c522fe:44183/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar with timestamp 1761922425913
[2025-10-31T14:53:46.873+0000] {subprocess.py:106} INFO - 25/10/31 14:53:46 INFO SparkContext: Added file file:///home/spark/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar at spark://68d0d8c522fe:44183/files/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar with timestamp 1761922425913
[2025-10-31T14:53:46.875+0000] {subprocess.py:106} INFO - 25/10/31 14:53:46 INFO Utils: Copying /home/spark/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar to /tmp/spark-c1db61f7-fd68-41f4-bfb5-55cc0045976d/userFiles-d1b7f436-c4b7-4fd3-be47-a8a00c035883/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar
[2025-10-31T14:53:47.091+0000] {subprocess.py:106} INFO - 25/10/31 14:53:47 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2025-10-31T14:53:47.161+0000] {subprocess.py:106} INFO - 25/10/31 14:53:47 INFO TransportClientFactory: Successfully created connection to spark-master/172.30.0.6:7077 after 48 ms (0 ms spent in bootstraps)
[2025-10-31T14:53:47.271+0000] {subprocess.py:106} INFO - 25/10/31 14:53:47 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20251031145347-0006
[2025-10-31T14:53:47.275+0000] {subprocess.py:106} INFO - 25/10/31 14:53:47 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20251031145347-0006/0 on worker-20251031122959-172.30.0.7-38595 (172.30.0.7:38595) with 1 core(s)
[2025-10-31T14:53:47.276+0000] {subprocess.py:106} INFO - 25/10/31 14:53:47 INFO StandaloneSchedulerBackend: Granted executor ID app-20251031145347-0006/0 on hostPort 172.30.0.7:38595 with 1 core(s), 1024.0 MiB RAM
[2025-10-31T14:53:47.284+0000] {subprocess.py:106} INFO - 25/10/31 14:53:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34675.
[2025-10-31T14:53:47.285+0000] {subprocess.py:106} INFO - 25/10/31 14:53:47 INFO NettyBlockTransferService: Server created on 68d0d8c522fe:34675
[2025-10-31T14:53:47.288+0000] {subprocess.py:106} INFO - 25/10/31 14:53:47 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-10-31T14:53:47.295+0000] {subprocess.py:106} INFO - 25/10/31 14:53:47 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 68d0d8c522fe, 34675, None)
[2025-10-31T14:53:47.300+0000] {subprocess.py:106} INFO - 25/10/31 14:53:47 INFO BlockManagerMasterEndpoint: Registering block manager 68d0d8c522fe:34675 with 434.4 MiB RAM, BlockManagerId(driver, 68d0d8c522fe, 34675, None)
[2025-10-31T14:53:47.303+0000] {subprocess.py:106} INFO - 25/10/31 14:53:47 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 68d0d8c522fe, 34675, None)
[2025-10-31T14:53:47.310+0000] {subprocess.py:106} INFO - 25/10/31 14:53:47 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 68d0d8c522fe, 34675, None)
[2025-10-31T14:53:47.365+0000] {subprocess.py:106} INFO - 25/10/31 14:53:47 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20251031145347-0006/0 is now RUNNING
[2025-10-31T14:53:47.637+0000] {subprocess.py:106} INFO - 25/10/31 14:53:47 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2025-10-31T14:53:47.942+0000] {subprocess.py:106} INFO - 25/10/31 14:53:47 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-10-31T14:53:47.947+0000] {subprocess.py:106} INFO - 25/10/31 14:53:47 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
[2025-10-31T14:53:50.140+0000] {subprocess.py:106} INFO - 25/10/31 14:53:50 INFO InMemoryFileIndex: It took 86 ms to list leaf files for 1 paths.
[2025-10-31T14:53:50.329+0000] {subprocess.py:106} INFO - 25/10/31 14:53:50 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 203.7 KiB, free 434.2 MiB)
[2025-10-31T14:53:50.412+0000] {subprocess.py:106} INFO - 25/10/31 14:53:50 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.3 KiB, free 434.2 MiB)
[2025-10-31T14:53:50.417+0000] {subprocess.py:106} INFO - 25/10/31 14:53:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 68d0d8c522fe:34675 (size: 35.3 KiB, free: 434.4 MiB)
[2025-10-31T14:53:50.424+0000] {subprocess.py:106} INFO - 25/10/31 14:53:50 INFO SparkContext: Created broadcast 0 from csv at <unknown>:0
[2025-10-31T14:53:50.882+0000] {subprocess.py:106} INFO - 25/10/31 14:53:50 INFO FileInputFormat: Total input files to process : 1
[2025-10-31T14:53:50.888+0000] {subprocess.py:106} INFO - 25/10/31 14:53:50 INFO FileInputFormat: Total input files to process : 1
[2025-10-31T14:53:50.975+0000] {subprocess.py:106} INFO - 25/10/31 14:53:50 INFO SparkContext: Starting job: csv at <unknown>:0
[2025-10-31T14:53:50.980+0000] {subprocess.py:106} INFO - 25/10/31 14:53:50 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.30.0.7:44956) with ID 0,  ResourceProfileId 0
[2025-10-31T14:53:51.007+0000] {subprocess.py:106} INFO - 25/10/31 14:53:51 INFO DAGScheduler: Got job 0 (csv at <unknown>:0) with 1 output partitions
[2025-10-31T14:53:51.008+0000] {subprocess.py:106} INFO - 25/10/31 14:53:51 INFO DAGScheduler: Final stage: ResultStage 0 (csv at <unknown>:0)
[2025-10-31T14:53:51.009+0000] {subprocess.py:106} INFO - 25/10/31 14:53:51 INFO DAGScheduler: Parents of final stage: List()
[2025-10-31T14:53:51.012+0000] {subprocess.py:106} INFO - 25/10/31 14:53:51 INFO DAGScheduler: Missing parents: List()
[2025-10-31T14:53:51.023+0000] {subprocess.py:106} INFO - 25/10/31 14:53:51 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at <unknown>:0), which has no missing parents
[2025-10-31T14:53:51.050+0000] {subprocess.py:106} INFO - 25/10/31 14:53:51 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.6 KiB, free 434.2 MiB)
[2025-10-31T14:53:51.054+0000] {subprocess.py:106} INFO - 25/10/31 14:53:51 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 434.2 MiB)
[2025-10-31T14:53:51.055+0000] {subprocess.py:106} INFO - 25/10/31 14:53:51 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 68d0d8c522fe:34675 (size: 4.4 KiB, free: 434.4 MiB)
[2025-10-31T14:53:51.057+0000] {subprocess.py:106} INFO - 25/10/31 14:53:51 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
[2025-10-31T14:53:51.090+0000] {subprocess.py:106} INFO - 25/10/31 14:53:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-10-31T14:53:51.092+0000] {subprocess.py:106} INFO - 25/10/31 14:53:51 INFO BlockManagerMasterEndpoint: Registering block manager 172.30.0.7:35695 with 434.4 MiB RAM, BlockManagerId(0, 172.30.0.7, 35695, None)
[2025-10-31T14:53:51.093+0000] {subprocess.py:106} INFO - 25/10/31 14:53:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-10-31T14:53:51.735+0000] {subprocess.py:106} INFO - 25/10/31 14:53:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.30.0.7, executor 0, partition 0, PROCESS_LOCAL, 9484 bytes)
[2025-10-31T14:53:51.977+0000] {subprocess.py:106} INFO - 25/10/31 14:53:51 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.30.0.7:35695 (size: 4.4 KiB, free: 434.4 MiB)
[2025-10-31T14:53:52.199+0000] {subprocess.py:106} INFO - 25/10/31 14:53:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.30.0.7:35695 (size: 35.3 KiB, free: 434.4 MiB)
[2025-10-31T14:53:53.236+0000] {subprocess.py:106} INFO - 25/10/31 14:53:53 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1518 ms on 172.30.0.7 (executor 0) (1/1)
[2025-10-31T14:53:53.239+0000] {subprocess.py:106} INFO - 25/10/31 14:53:53 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-10-31T14:53:53.248+0000] {subprocess.py:106} INFO - 25/10/31 14:53:53 INFO DAGScheduler: ResultStage 0 (csv at <unknown>:0) finished in 2.205 s
[2025-10-31T14:53:53.254+0000] {subprocess.py:106} INFO - 25/10/31 14:53:53 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-10-31T14:53:53.255+0000] {subprocess.py:106} INFO - 25/10/31 14:53:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-10-31T14:53:53.257+0000] {subprocess.py:106} INFO - 25/10/31 14:53:53 INFO DAGScheduler: Job 0 finished: csv at <unknown>:0, took 2.282182 s
[2025-10-31T14:53:53.600+0000] {subprocess.py:106} INFO - 25/10/31 14:53:53 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 68d0d8c522fe:34675 in memory (size: 35.3 KiB, free: 434.4 MiB)
[2025-10-31T14:53:53.616+0000] {subprocess.py:106} INFO - 25/10/31 14:53:53 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.30.0.7:35695 in memory (size: 35.3 KiB, free: 434.4 MiB)
[2025-10-31T14:53:53.643+0000] {subprocess.py:106} INFO - 25/10/31 14:53:53 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 68d0d8c522fe:34675 in memory (size: 4.4 KiB, free: 434.4 MiB)
[2025-10-31T14:53:53.646+0000] {subprocess.py:106} INFO - 25/10/31 14:53:53 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.30.0.7:35695 in memory (size: 4.4 KiB, free: 434.4 MiB)
[2025-10-31T14:53:55.151+0000] {subprocess.py:106} INFO - 25/10/31 14:53:55 INFO FileSourceStrategy: Pushed Filters:
[2025-10-31T14:53:55.153+0000] {subprocess.py:106} INFO - 25/10/31 14:53:55 INFO FileSourceStrategy: Post-Scan Filters:
[2025-10-31T14:53:55.740+0000] {subprocess.py:106} INFO - 25/10/31 14:53:55 INFO CodeGenerator: Code generated in 332.252353 ms
[2025-10-31T14:53:55.756+0000] {subprocess.py:106} INFO - 25/10/31 14:53:55 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 203.1 KiB, free 434.2 MiB)
[2025-10-31T14:53:55.788+0000] {subprocess.py:106} INFO - 25/10/31 14:53:55 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 434.2 MiB)
[2025-10-31T14:53:55.792+0000] {subprocess.py:106} INFO - 25/10/31 14:53:55 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 68d0d8c522fe:34675 (size: 35.1 KiB, free: 434.4 MiB)
[2025-10-31T14:53:55.794+0000] {subprocess.py:106} INFO - 25/10/31 14:53:55 INFO SparkContext: Created broadcast 2 from count at <unknown>:0
[2025-10-31T14:53:55.828+0000] {subprocess.py:106} INFO - 25/10/31 14:53:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-10-31T14:53:55.897+0000] {subprocess.py:106} INFO - 25/10/31 14:53:55 INFO DAGScheduler: Registering RDD 7 (count at <unknown>:0) as input to shuffle 0
[2025-10-31T14:53:55.903+0000] {subprocess.py:106} INFO - 25/10/31 14:53:55 INFO DAGScheduler: Got map stage job 1 (count at <unknown>:0) with 1 output partitions
[2025-10-31T14:53:55.904+0000] {subprocess.py:106} INFO - 25/10/31 14:53:55 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (count at <unknown>:0)
[2025-10-31T14:53:55.905+0000] {subprocess.py:106} INFO - 25/10/31 14:53:55 INFO DAGScheduler: Parents of final stage: List()
[2025-10-31T14:53:55.908+0000] {subprocess.py:106} INFO - 25/10/31 14:53:55 INFO DAGScheduler: Missing parents: List()
[2025-10-31T14:53:55.915+0000] {subprocess.py:106} INFO - 25/10/31 14:53:55 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0), which has no missing parents
[2025-10-31T14:53:55.975+0000] {subprocess.py:106} INFO - 25/10/31 14:53:55 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 18.0 KiB, free 434.1 MiB)
[2025-10-31T14:53:55.991+0000] {subprocess.py:106} INFO - 25/10/31 14:53:55 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.1 MiB)
[2025-10-31T14:53:55.994+0000] {subprocess.py:106} INFO - 25/10/31 14:53:55 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 68d0d8c522fe:34675 (size: 8.9 KiB, free: 434.4 MiB)
[2025-10-31T14:53:55.996+0000] {subprocess.py:106} INFO - 25/10/31 14:53:55 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
[2025-10-31T14:53:55.997+0000] {subprocess.py:106} INFO - 25/10/31 14:53:55 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-10-31T14:53:55.998+0000] {subprocess.py:106} INFO - 25/10/31 14:53:55 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2025-10-31T14:53:56.012+0000] {subprocess.py:106} INFO - 25/10/31 14:53:56 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.30.0.7, executor 0, partition 0, ANY, 9944 bytes)
[2025-10-31T14:53:56.081+0000] {subprocess.py:106} INFO - 25/10/31 14:53:56 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.30.0.7:35695 (size: 8.9 KiB, free: 434.4 MiB)
[2025-10-31T14:53:56.892+0000] {subprocess.py:106} INFO - 25/10/31 14:53:56 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.30.0.7:35695 (size: 35.1 KiB, free: 434.4 MiB)
[2025-10-31T14:53:57.078+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1073 ms on 172.30.0.7 (executor 0) (1/1)
[2025-10-31T14:53:57.078+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-10-31T14:53:57.082+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO DAGScheduler: ShuffleMapStage 1 (count at <unknown>:0) finished in 1.161 s
[2025-10-31T14:53:57.084+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO DAGScheduler: looking for newly runnable stages
[2025-10-31T14:53:57.085+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO DAGScheduler: running: Set()
[2025-10-31T14:53:57.086+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO DAGScheduler: waiting: Set()
[2025-10-31T14:53:57.086+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO DAGScheduler: failed: Set()
[2025-10-31T14:53:57.144+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO CodeGenerator: Code generated in 21.13514 ms
[2025-10-31T14:53:57.182+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO SparkContext: Starting job: count at <unknown>:0
[2025-10-31T14:53:57.186+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO DAGScheduler: Got job 2 (count at <unknown>:0) with 1 output partitions
[2025-10-31T14:53:57.187+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO DAGScheduler: Final stage: ResultStage 3 (count at <unknown>:0)
[2025-10-31T14:53:57.187+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
[2025-10-31T14:53:57.188+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO DAGScheduler: Missing parents: List()
[2025-10-31T14:53:57.189+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0), which has no missing parents
[2025-10-31T14:53:57.199+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 12.5 KiB, free 434.1 MiB)
[2025-10-31T14:53:57.214+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.1 MiB)
[2025-10-31T14:53:57.218+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 68d0d8c522fe:34675 (size: 5.9 KiB, free: 434.4 MiB)
[2025-10-31T14:53:57.221+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
[2025-10-31T14:53:57.222+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 68d0d8c522fe:34675 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2025-10-31T14:53:57.223+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-10-31T14:53:57.225+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2025-10-31T14:53:57.226+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.30.0.7:35695 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2025-10-31T14:53:57.232+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (172.30.0.7, executor 0, partition 0, NODE_LOCAL, 9331 bytes)
[2025-10-31T14:53:57.276+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.30.0.7:35695 (size: 5.9 KiB, free: 434.4 MiB)
[2025-10-31T14:53:57.318+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.30.0.7:44956
[2025-10-31T14:53:57.481+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 251 ms on 172.30.0.7 (executor 0) (1/1)
[2025-10-31T14:53:57.482+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2025-10-31T14:53:57.483+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO DAGScheduler: ResultStage 3 (count at <unknown>:0) finished in 0.285 s
[2025-10-31T14:53:57.484+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-10-31T14:53:57.484+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2025-10-31T14:53:57.485+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO DAGScheduler: Job 2 finished: count at <unknown>:0, took 0.300766 s
[2025-10-31T14:53:57.492+0000] {subprocess.py:106} INFO -  Record loaded from CSV: 3433
[2025-10-31T14:53:57.496+0000] {subprocess.py:106} INFO - root
[2025-10-31T14:53:57.497+0000] {subprocess.py:106} INFO -  |-- job_title: string (nullable = true)
[2025-10-31T14:53:57.498+0000] {subprocess.py:106} INFO -  |-- company_name: string (nullable = true)
[2025-10-31T14:53:57.498+0000] {subprocess.py:106} INFO -  |-- location: string (nullable = true)
[2025-10-31T14:53:57.499+0000] {subprocess.py:106} INFO -  |-- skills_required: string (nullable = true)
[2025-10-31T14:53:57.500+0000] {subprocess.py:106} INFO -  |-- date_posted: string (nullable = true)
[2025-10-31T14:53:57.500+0000] {subprocess.py:106} INFO -  |-- job_link: string (nullable = true)
[2025-10-31T14:53:57.501+0000] {subprocess.py:106} INFO -  |-- job_category: string (nullable = true)
[2025-10-31T14:53:57.502+0000] {subprocess.py:106} INFO -  |-- work_mode: string (nullable = true)
[2025-10-31T14:53:57.502+0000] {subprocess.py:106} INFO -  |-- crawl_date: string (nullable = true)
[2025-10-31T14:53:57.503+0000] {subprocess.py:106} INFO -  |-- date_only: string (nullable = true)
[2025-10-31T14:53:57.503+0000] {subprocess.py:106} INFO - 
[2025-10-31T14:53:57.790+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 68d0d8c522fe:34675 in memory (size: 5.9 KiB, free: 434.4 MiB)
[2025-10-31T14:53:57.794+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.30.0.7:35695 in memory (size: 5.9 KiB, free: 434.4 MiB)
[2025-10-31T14:53:57.876+0000] {subprocess.py:106} INFO - 25/10/31 14:53:57 INFO HiveConf: Found configuration file null
[2025-10-31T14:53:58.044+0000] {subprocess.py:106} INFO - 25/10/31 14:53:58 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
[2025-10-31T14:53:58.068+0000] {subprocess.py:106} INFO - 25/10/31 14:53:58 INFO metastore: Opened a connection to metastore, current connections: 1
[2025-10-31T14:53:58.100+0000] {subprocess.py:106} INFO - 25/10/31 14:53:58 INFO metastore: Connected to metastore.
[2025-10-31T14:53:58.462+0000] {subprocess.py:106} INFO - 25/10/31 14:53:58 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: hdfs://dinhhoa-master:9000/user/ndh/warehouse/bronze.db/it_jobs_raw/metadata/00000-5f98341f-8c6b-4263-b5b9-f76ab8c7f9a5.metadata.json
[2025-10-31T14:53:58.718+0000] {subprocess.py:106} INFO - 25/10/31 14:53:58 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 68d0d8c522fe:34675 in memory (size: 35.1 KiB, free: 434.4 MiB)
[2025-10-31T14:53:58.722+0000] {subprocess.py:106} INFO - 25/10/31 14:53:58 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.30.0.7:35695 in memory (size: 35.1 KiB, free: 434.4 MiB)
[2025-10-31T14:53:58.870+0000] {subprocess.py:106} INFO - 25/10/31 14:53:58 INFO BaseMetastoreCatalog: Table loaded by catalog: hive_catalog.bronze.it_jobs_raw
[2025-10-31T14:53:59.131+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO V2ScanRelationPushDown:
[2025-10-31T14:53:59.131+0000] {subprocess.py:106} INFO - Pushing operators to hive_catalog.bronze.it_jobs_raw
[2025-10-31T14:53:59.132+0000] {subprocess.py:106} INFO - Pushed Filters: job_link IS NOT NULL
[2025-10-31T14:53:59.132+0000] {subprocess.py:106} INFO - Post-Scan Filters: isnotnull(job_link#66)
[2025-10-31T14:53:59.133+0000] {subprocess.py:106} INFO - 
[2025-10-31T14:53:59.163+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO V2ScanRelationPushDown:
[2025-10-31T14:53:59.163+0000] {subprocess.py:106} INFO - Output: job_link#66
[2025-10-31T14:53:59.163+0000] {subprocess.py:106} INFO - 
[2025-10-31T14:53:59.173+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO SnapshotScan: Scanning table hive_catalog.bronze.it_jobs_raw snapshot 2601246677096618756 created at 2025-10-30T09:59:22.969+00:00 with filter job_link IS NOT NULL
[2025-10-31T14:53:59.342+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO BaseDistributedDataScan: Planning file tasks locally for table hive_catalog.bronze.it_jobs_raw
[2025-10-31T14:53:59.491+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table hive_catalog.bronze.it_jobs_raw
[2025-10-31T14:53:59.553+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO FileSourceStrategy: Pushed Filters:
[2025-10-31T14:53:59.554+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO FileSourceStrategy: Post-Scan Filters:
[2025-10-31T14:53:59.569+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 32.0 KiB, free 434.4 MiB)
[2025-10-31T14:53:59.592+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 29.7 KiB, free 434.3 MiB)
[2025-10-31T14:53:59.593+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 68d0d8c522fe:34675 (size: 29.7 KiB, free: 434.4 MiB)
[2025-10-31T14:53:59.594+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO SparkContext: Created broadcast 5 from broadcast at SparkBatch.java:79
[2025-10-31T14:53:59.632+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-10-31T14:53:59.636+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 29.7 KiB, free 434.3 MiB)
[2025-10-31T14:53:59.638+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 68d0d8c522fe:34675 (size: 29.7 KiB, free: 434.3 MiB)
[2025-10-31T14:53:59.638+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO SparkContext: Created broadcast 6 from broadcast at SparkBatch.java:79
[2025-10-31T14:53:59.726+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO CodeGenerator: Code generated in 22.343143 ms
[2025-10-31T14:53:59.765+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at <unknown>:0
[2025-10-31T14:53:59.768+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO DAGScheduler: Got job 3 ($anonfun$withThreadLocalCaptured$1 at <unknown>:0) with 1 output partitions
[2025-10-31T14:53:59.770+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at <unknown>:0)
[2025-10-31T14:53:59.771+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO DAGScheduler: Parents of final stage: List()
[2025-10-31T14:53:59.772+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO DAGScheduler: Missing parents: List()
[2025-10-31T14:53:59.773+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[14] at $anonfun$withThreadLocalCaptured$1 at <unknown>:0), which has no missing parents
[2025-10-31T14:53:59.773+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 13.8 KiB, free 434.3 MiB)
[2025-10-31T14:53:59.788+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.3 MiB)
[2025-10-31T14:53:59.794+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 68d0d8c522fe:34675 (size: 5.5 KiB, free: 434.3 MiB)
[2025-10-31T14:53:59.795+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
[2025-10-31T14:53:59.796+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at $anonfun$withThreadLocalCaptured$1 at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-10-31T14:53:59.798+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2025-10-31T14:53:59.822+0000] {subprocess.py:106} INFO - 25/10/31 14:53:59 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (172.30.0.7, executor 0, partition 0, ANY, 14330 bytes)
[2025-10-31T14:54:00.297+0000] {subprocess.py:106} INFO - 25/10/31 14:54:00 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.30.0.7:35695 (size: 5.5 KiB, free: 434.4 MiB)
[2025-10-31T14:54:00.362+0000] {subprocess.py:106} INFO - 25/10/31 14:54:00 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.30.0.7:35695 (size: 29.7 KiB, free: 434.4 MiB)
[2025-10-31T14:54:01.509+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 1713 ms on 172.30.0.7 (executor 0) (1/1)
[2025-10-31T14:54:01.510+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2025-10-31T14:54:01.511+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at <unknown>:0) finished in 1.739 s
[2025-10-31T14:54:01.512+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-10-31T14:54:01.512+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2025-10-31T14:54:01.513+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO DAGScheduler: Job 3 finished: $anonfun$withThreadLocalCaptured$1 at <unknown>:0, took 1.745883 s
[2025-10-31T14:54:01.547+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO CodeGenerator: Code generated in 11.092671 ms
[2025-10-31T14:54:01.563+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 68d0d8c522fe:34675 in memory (size: 5.5 KiB, free: 434.3 MiB)
[2025-10-31T14:54:01.564+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.30.0.7:35695 in memory (size: 5.5 KiB, free: 434.4 MiB)
[2025-10-31T14:54:01.624+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 186.5 KiB, free 434.1 MiB)
[2025-10-31T14:54:01.625+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 68d0d8c522fe:34675 (size: 186.5 KiB, free: 434.2 MiB)
[2025-10-31T14:54:01.627+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO SparkContext: Created broadcast 8 from $anonfun$withThreadLocalCaptured$1 at <unknown>:0
[2025-10-31T14:54:01.639+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO FileSourceStrategy: Pushed Filters:
[2025-10-31T14:54:01.640+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO FileSourceStrategy: Post-Scan Filters:
[2025-10-31T14:54:01.694+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO CodeGenerator: Code generated in 16.713388 ms
[2025-10-31T14:54:01.699+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 203.1 KiB, free 433.9 MiB)
[2025-10-31T14:54:01.721+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 433.9 MiB)
[2025-10-31T14:54:01.722+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 68d0d8c522fe:34675 (size: 35.1 KiB, free: 434.1 MiB)
[2025-10-31T14:54:01.724+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO SparkContext: Created broadcast 9 from count at <unknown>:0
[2025-10-31T14:54:01.726+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-10-31T14:54:01.738+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO DAGScheduler: Registering RDD 18 (count at <unknown>:0) as input to shuffle 1
[2025-10-31T14:54:01.739+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO DAGScheduler: Got map stage job 4 (count at <unknown>:0) with 1 output partitions
[2025-10-31T14:54:01.740+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (count at <unknown>:0)
[2025-10-31T14:54:01.741+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO DAGScheduler: Parents of final stage: List()
[2025-10-31T14:54:01.742+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO DAGScheduler: Missing parents: List()
[2025-10-31T14:54:01.745+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[18] at count at <unknown>:0), which has no missing parents
[2025-10-31T14:54:01.750+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 20.1 KiB, free 433.8 MiB)
[2025-10-31T14:54:01.761+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 433.8 MiB)
[2025-10-31T14:54:01.764+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 68d0d8c522fe:34675 (size: 9.7 KiB, free: 434.1 MiB)
[2025-10-31T14:54:01.764+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585
[2025-10-31T14:54:01.766+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[18] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-10-31T14:54:01.767+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2025-10-31T14:54:01.768+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (172.30.0.7, executor 0, partition 0, ANY, 9944 bytes)
[2025-10-31T14:54:01.789+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.30.0.7:35695 (size: 9.7 KiB, free: 434.4 MiB)
[2025-10-31T14:54:01.864+0000] {subprocess.py:106} INFO - 25/10/31 14:54:01 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.30.0.7:35695 (size: 186.5 KiB, free: 434.2 MiB)
[2025-10-31T14:54:02.037+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.30.0.7:35695 (size: 35.1 KiB, free: 434.1 MiB)
[2025-10-31T14:54:02.139+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 371 ms on 172.30.0.7 (executor 0) (1/1)
[2025-10-31T14:54:02.140+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2025-10-31T14:54:02.141+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO DAGScheduler: ShuffleMapStage 5 (count at <unknown>:0) finished in 0.394 s
[2025-10-31T14:54:02.142+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO DAGScheduler: looking for newly runnable stages
[2025-10-31T14:54:02.142+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO DAGScheduler: running: Set()
[2025-10-31T14:54:02.143+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO DAGScheduler: waiting: Set()
[2025-10-31T14:54:02.144+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO DAGScheduler: failed: Set()
[2025-10-31T14:54:02.167+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO CodeGenerator: Code generated in 11.571843 ms
[2025-10-31T14:54:02.189+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO SparkContext: Starting job: count at <unknown>:0
[2025-10-31T14:54:02.192+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO DAGScheduler: Got job 5 (count at <unknown>:0) with 1 output partitions
[2025-10-31T14:54:02.193+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO DAGScheduler: Final stage: ResultStage 7 (count at <unknown>:0)
[2025-10-31T14:54:02.194+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
[2025-10-31T14:54:02.196+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO DAGScheduler: Missing parents: List()
[2025-10-31T14:54:02.196+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[21] at count at <unknown>:0), which has no missing parents
[2025-10-31T14:54:02.197+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 12.5 KiB, free 433.8 MiB)
[2025-10-31T14:54:02.210+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.8 MiB)
[2025-10-31T14:54:02.214+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 68d0d8c522fe:34675 (size: 5.9 KiB, free: 434.1 MiB)
[2025-10-31T14:54:02.214+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585
[2025-10-31T14:54:02.216+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 68d0d8c522fe:34675 in memory (size: 9.7 KiB, free: 434.1 MiB)
[2025-10-31T14:54:02.217+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[21] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-10-31T14:54:02.218+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2025-10-31T14:54:02.219+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.30.0.7:35695 in memory (size: 9.7 KiB, free: 434.2 MiB)
[2025-10-31T14:54:02.220+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 5) (172.30.0.7, executor 0, partition 0, NODE_LOCAL, 9331 bytes)
[2025-10-31T14:54:02.248+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.30.0.7:35695 (size: 5.9 KiB, free: 434.1 MiB)
[2025-10-31T14:54:02.268+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.30.0.7:44956
[2025-10-31T14:54:02.316+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 5) in 99 ms on 172.30.0.7 (executor 0) (1/1)
[2025-10-31T14:54:02.317+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2025-10-31T14:54:02.319+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO DAGScheduler: ResultStage 7 (count at <unknown>:0) finished in 0.123 s
[2025-10-31T14:54:02.319+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-10-31T14:54:02.320+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2025-10-31T14:54:02.321+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO DAGScheduler: Job 5 finished: count at <unknown>:0, took 0.131973 s
[2025-10-31T14:54:02.328+0000] {subprocess.py:106} INFO -  New records to append: 299
[2025-10-31T14:54:02.377+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO V2ScanRelationPushDown:
[2025-10-31T14:54:02.378+0000] {subprocess.py:106} INFO - Pushing operators to hive_catalog.bronze.it_jobs_raw
[2025-10-31T14:54:02.379+0000] {subprocess.py:106} INFO - Pushed Filters: job_link IS NOT NULL
[2025-10-31T14:54:02.380+0000] {subprocess.py:106} INFO - Post-Scan Filters: isnotnull(job_link#66)
[2025-10-31T14:54:02.380+0000] {subprocess.py:106} INFO - 
[2025-10-31T14:54:02.381+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO V2ScanRelationPushDown:
[2025-10-31T14:54:02.382+0000] {subprocess.py:106} INFO - Output: job_link#66
[2025-10-31T14:54:02.383+0000] {subprocess.py:106} INFO - 
[2025-10-31T14:54:02.384+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO SnapshotScan: Scanning table hive_catalog.bronze.it_jobs_raw snapshot 2601246677096618756 created at 2025-10-30T09:59:22.969+00:00 with filter job_link IS NOT NULL
[2025-10-31T14:54:02.385+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO BaseDistributedDataScan: Planning file tasks locally for table hive_catalog.bronze.it_jobs_raw
[2025-10-31T14:54:02.420+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table hive_catalog.bronze.it_jobs_raw
[2025-10-31T14:54:02.433+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO FileSourceStrategy: Pushed Filters:
[2025-10-31T14:54:02.434+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO FileSourceStrategy: Post-Scan Filters:
[2025-10-31T14:54:02.438+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 32.0 KiB, free 433.8 MiB)
[2025-10-31T14:54:02.460+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 29.7 KiB, free 433.8 MiB)
[2025-10-31T14:54:02.462+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 68d0d8c522fe:34675 (size: 29.7 KiB, free: 434.1 MiB)
[2025-10-31T14:54:02.467+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO SparkContext: Created broadcast 12 from broadcast at SparkBatch.java:79
[2025-10-31T14:54:02.468+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 68d0d8c522fe:34675 in memory (size: 5.9 KiB, free: 434.1 MiB)
[2025-10-31T14:54:02.469+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.30.0.7:35695 in memory (size: 5.9 KiB, free: 434.2 MiB)
[2025-10-31T14:54:02.487+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 32.0 KiB, free 433.8 MiB)
[2025-10-31T14:54:02.491+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 29.7 KiB, free 433.7 MiB)
[2025-10-31T14:54:02.492+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 68d0d8c522fe:34675 (size: 29.7 KiB, free: 434.1 MiB)
[2025-10-31T14:54:02.495+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO SparkContext: Created broadcast 13 from broadcast at SparkBatch.java:79
[2025-10-31T14:54:02.577+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at <unknown>:0
[2025-10-31T14:54:02.580+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO DAGScheduler: Got job 6 ($anonfun$withThreadLocalCaptured$1 at <unknown>:0) with 1 output partitions
[2025-10-31T14:54:02.581+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO DAGScheduler: Final stage: ResultStage 8 ($anonfun$withThreadLocalCaptured$1 at <unknown>:0)
[2025-10-31T14:54:02.582+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO DAGScheduler: Parents of final stage: List()
[2025-10-31T14:54:02.583+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO DAGScheduler: Missing parents: List()
[2025-10-31T14:54:02.585+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[25] at $anonfun$withThreadLocalCaptured$1 at <unknown>:0), which has no missing parents
[2025-10-31T14:54:02.586+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 13.7 KiB, free 433.7 MiB)
[2025-10-31T14:54:02.608+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 433.7 MiB)
[2025-10-31T14:54:02.610+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 68d0d8c522fe:34675 (size: 5.5 KiB, free: 434.1 MiB)
[2025-10-31T14:54:02.613+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1585
[2025-10-31T14:54:02.614+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[25] at $anonfun$withThreadLocalCaptured$1 at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-10-31T14:54:02.615+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2025-10-31T14:54:02.620+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (172.30.0.7, executor 0, partition 0, ANY, 14330 bytes)
[2025-10-31T14:54:02.642+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.30.0.7:35695 (size: 5.5 KiB, free: 434.1 MiB)
[2025-10-31T14:54:02.674+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.30.0.7:35695 (size: 29.7 KiB, free: 434.1 MiB)
[2025-10-31T14:54:02.947+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 331 ms on 172.30.0.7 (executor 0) (1/1)
[2025-10-31T14:54:02.950+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2025-10-31T14:54:02.953+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO DAGScheduler: ResultStage 8 ($anonfun$withThreadLocalCaptured$1 at <unknown>:0) finished in 0.366 s
[2025-10-31T14:54:02.955+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-10-31T14:54:02.957+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2025-10-31T14:54:02.959+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO DAGScheduler: Job 6 finished: $anonfun$withThreadLocalCaptured$1 at <unknown>:0, took 0.373530 s
[2025-10-31T14:54:03.001+0000] {subprocess.py:106} INFO - 25/10/31 14:54:02 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 68d0d8c522fe:34675 in memory (size: 5.5 KiB, free: 434.1 MiB)
[2025-10-31T14:54:03.007+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 186.5 KiB, free 433.6 MiB)
[2025-10-31T14:54:03.008+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 68d0d8c522fe:34675 (size: 186.5 KiB, free: 433.9 MiB)
[2025-10-31T14:54:03.013+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 172.30.0.7:35695 in memory (size: 5.5 KiB, free: 434.1 MiB)
[2025-10-31T14:54:03.014+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO SparkContext: Created broadcast 15 from $anonfun$withThreadLocalCaptured$1 at <unknown>:0
[2025-10-31T14:54:03.046+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO FileSourceStrategy: Pushed Filters:
[2025-10-31T14:54:03.047+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO FileSourceStrategy: Post-Scan Filters:
[2025-10-31T14:54:03.106+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 203.1 KiB, free 433.4 MiB)
[2025-10-31T14:54:03.144+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 433.3 MiB)
[2025-10-31T14:54:03.146+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 68d0d8c522fe:34675 (size: 35.1 KiB, free: 433.9 MiB)
[2025-10-31T14:54:03.151+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO SparkContext: Created broadcast 16 from count at <unknown>:0
[2025-10-31T14:54:03.158+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-10-31T14:54:03.164+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO DAGScheduler: Registering RDD 29 (count at <unknown>:0) as input to shuffle 2
[2025-10-31T14:54:03.166+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO DAGScheduler: Got map stage job 7 (count at <unknown>:0) with 1 output partitions
[2025-10-31T14:54:03.167+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (count at <unknown>:0)
[2025-10-31T14:54:03.167+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO DAGScheduler: Parents of final stage: List()
[2025-10-31T14:54:03.169+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO DAGScheduler: Missing parents: List()
[2025-10-31T14:54:03.173+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[29] at count at <unknown>:0), which has no missing parents
[2025-10-31T14:54:03.183+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 20.1 KiB, free 433.3 MiB)
[2025-10-31T14:54:03.184+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 9.7 KiB, free 433.3 MiB)
[2025-10-31T14:54:03.186+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 68d0d8c522fe:34675 (size: 9.7 KiB, free: 433.8 MiB)
[2025-10-31T14:54:03.189+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1585
[2025-10-31T14:54:03.190+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[29] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-10-31T14:54:03.191+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2025-10-31T14:54:03.193+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (172.30.0.7, executor 0, partition 0, ANY, 9944 bytes)
[2025-10-31T14:54:03.245+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.30.0.7:35695 (size: 9.7 KiB, free: 434.1 MiB)
[2025-10-31T14:54:03.280+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.30.0.7:35695 (size: 186.5 KiB, free: 433.9 MiB)
[2025-10-31T14:54:03.350+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.30.0.7:35695 (size: 35.1 KiB, free: 433.9 MiB)
[2025-10-31T14:54:03.471+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 275 ms on 172.30.0.7 (executor 0) (1/1)
[2025-10-31T14:54:03.476+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2025-10-31T14:54:03.478+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO DAGScheduler: ShuffleMapStage 9 (count at <unknown>:0) finished in 0.298 s
[2025-10-31T14:54:03.485+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO DAGScheduler: looking for newly runnable stages
[2025-10-31T14:54:03.486+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO DAGScheduler: running: Set()
[2025-10-31T14:54:03.487+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO DAGScheduler: waiting: Set()
[2025-10-31T14:54:03.487+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO DAGScheduler: failed: Set()
[2025-10-31T14:54:03.513+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO SparkContext: Starting job: count at <unknown>:0
[2025-10-31T14:54:03.515+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO DAGScheduler: Got job 8 (count at <unknown>:0) with 1 output partitions
[2025-10-31T14:54:03.520+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO DAGScheduler: Final stage: ResultStage 11 (count at <unknown>:0)
[2025-10-31T14:54:03.521+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
[2025-10-31T14:54:03.522+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO DAGScheduler: Missing parents: List()
[2025-10-31T14:54:03.523+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[32] at count at <unknown>:0), which has no missing parents
[2025-10-31T14:54:03.528+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 12.5 KiB, free 433.3 MiB)
[2025-10-31T14:54:03.547+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.3 MiB)
[2025-10-31T14:54:03.550+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 68d0d8c522fe:34675 (size: 5.9 KiB, free: 433.8 MiB)
[2025-10-31T14:54:03.552+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585
[2025-10-31T14:54:03.553+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[32] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-10-31T14:54:03.554+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2025-10-31T14:54:03.558+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.30.0.7:35695 in memory (size: 9.7 KiB, free: 433.9 MiB)
[2025-10-31T14:54:03.562+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 8) (172.30.0.7, executor 0, partition 0, NODE_LOCAL, 9331 bytes)
[2025-10-31T14:54:03.563+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 68d0d8c522fe:34675 in memory (size: 9.7 KiB, free: 433.8 MiB)
[2025-10-31T14:54:03.581+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.30.0.7:35695 (size: 5.9 KiB, free: 433.9 MiB)
[2025-10-31T14:54:03.590+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.30.0.7:44956
[2025-10-31T14:54:03.624+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 8) in 67 ms on 172.30.0.7 (executor 0) (1/1)
[2025-10-31T14:54:03.625+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2025-10-31T14:54:03.628+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO DAGScheduler: ResultStage 11 (count at <unknown>:0) finished in 0.109 s
[2025-10-31T14:54:03.629+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-10-31T14:54:03.630+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
[2025-10-31T14:54:03.630+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO DAGScheduler: Job 8 finished: count at <unknown>:0, took 0.116069 s
[2025-10-31T14:54:03.807+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO V2ScanRelationPushDown:
[2025-10-31T14:54:03.807+0000] {subprocess.py:106} INFO - Pushing operators to hive_catalog.bronze.it_jobs_raw
[2025-10-31T14:54:03.808+0000] {subprocess.py:106} INFO - Pushed Filters: job_link IS NOT NULL
[2025-10-31T14:54:03.809+0000] {subprocess.py:106} INFO - Post-Scan Filters: isnotnull(job_link#66)
[2025-10-31T14:54:03.811+0000] {subprocess.py:106} INFO - 
[2025-10-31T14:54:03.812+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO V2ScanRelationPushDown:
[2025-10-31T14:54:03.815+0000] {subprocess.py:106} INFO - Output: job_link#66
[2025-10-31T14:54:03.816+0000] {subprocess.py:106} INFO - 
[2025-10-31T14:54:03.817+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO SnapshotScan: Scanning table hive_catalog.bronze.it_jobs_raw snapshot 2601246677096618756 created at 2025-10-30T09:59:22.969+00:00 with filter job_link IS NOT NULL
[2025-10-31T14:54:03.818+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO BaseDistributedDataScan: Planning file tasks locally for table hive_catalog.bronze.it_jobs_raw
[2025-10-31T14:54:03.866+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table hive_catalog.bronze.it_jobs_raw
[2025-10-31T14:54:03.929+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table hive_catalog.bronze.it_jobs_raw
[2025-10-31T14:54:03.943+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table hive_catalog.bronze.it_jobs_raw
[2025-10-31T14:54:03.958+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO SparkWrite: Requesting [] as write ordering for table hive_catalog.bronze.it_jobs_raw
[2025-10-31T14:54:03.989+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO FileSourceStrategy: Pushed Filters:
[2025-10-31T14:54:03.990+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO FileSourceStrategy: Post-Scan Filters:
[2025-10-31T14:54:03.996+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 32.0 KiB, free 433.3 MiB)
[2025-10-31T14:54:03.999+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 29.7 KiB, free 433.3 MiB)
[2025-10-31T14:54:04.001+0000] {subprocess.py:106} INFO - 25/10/31 14:54:03 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 68d0d8c522fe:34675 (size: 29.7 KiB, free: 433.8 MiB)
[2025-10-31T14:54:04.003+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO SparkContext: Created broadcast 19 from broadcast at SparkBatch.java:79
[2025-10-31T14:54:04.023+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 32.0 KiB, free 433.2 MiB)
[2025-10-31T14:54:04.027+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 29.7 KiB, free 433.2 MiB)
[2025-10-31T14:54:04.029+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 68d0d8c522fe:34675 (size: 29.7 KiB, free: 433.8 MiB)
[2025-10-31T14:54:04.031+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO SparkContext: Created broadcast 20 from broadcast at SparkBatch.java:79
[2025-10-31T14:54:04.094+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at <unknown>:0
[2025-10-31T14:54:04.095+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO DAGScheduler: Got job 9 ($anonfun$withThreadLocalCaptured$1 at <unknown>:0) with 1 output partitions
[2025-10-31T14:54:04.096+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO DAGScheduler: Final stage: ResultStage 12 ($anonfun$withThreadLocalCaptured$1 at <unknown>:0)
[2025-10-31T14:54:04.097+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO DAGScheduler: Parents of final stage: List()
[2025-10-31T14:54:04.098+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO DAGScheduler: Missing parents: List()
[2025-10-31T14:54:04.099+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[36] at $anonfun$withThreadLocalCaptured$1 at <unknown>:0), which has no missing parents
[2025-10-31T14:54:04.101+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 13.7 KiB, free 433.2 MiB)
[2025-10-31T14:54:04.126+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 433.2 MiB)
[2025-10-31T14:54:04.128+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 68d0d8c522fe:34675 (size: 5.5 KiB, free: 433.8 MiB)
[2025-10-31T14:54:04.132+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1585
[2025-10-31T14:54:04.133+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[36] at $anonfun$withThreadLocalCaptured$1 at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-10-31T14:54:04.138+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2025-10-31T14:54:04.140+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 68d0d8c522fe:34675 in memory (size: 29.7 KiB, free: 433.8 MiB)
[2025-10-31T14:54:04.141+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 9) (172.30.0.7, executor 0, partition 0, ANY, 14330 bytes)
[2025-10-31T14:54:04.159+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 68d0d8c522fe:34675 in memory (size: 29.7 KiB, free: 433.8 MiB)
[2025-10-31T14:54:04.166+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.30.0.7:35695 in memory (size: 29.7 KiB, free: 433.9 MiB)
[2025-10-31T14:54:04.178+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.30.0.7:35695 (size: 5.5 KiB, free: 433.9 MiB)
[2025-10-31T14:54:04.191+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.30.0.7:35695 in memory (size: 5.9 KiB, free: 433.9 MiB)
[2025-10-31T14:54:04.193+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 68d0d8c522fe:34675 in memory (size: 5.9 KiB, free: 433.8 MiB)
[2025-10-31T14:54:04.237+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 68d0d8c522fe:34675 in memory (size: 35.1 KiB, free: 433.9 MiB)
[2025-10-31T14:54:04.240+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 172.30.0.7:35695 in memory (size: 35.1 KiB, free: 434.0 MiB)
[2025-10-31T14:54:04.244+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.30.0.7:35695 (size: 29.7 KiB, free: 433.9 MiB)
[2025-10-31T14:54:04.261+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 68d0d8c522fe:34675 in memory (size: 186.5 KiB, free: 434.1 MiB)
[2025-10-31T14:54:04.262+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.30.0.7:35695 in memory (size: 186.5 KiB, free: 434.1 MiB)
[2025-10-31T14:54:04.356+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 9) in 219 ms on 172.30.0.7 (executor 0) (1/1)
[2025-10-31T14:54:04.357+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2025-10-31T14:54:04.358+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO DAGScheduler: ResultStage 12 ($anonfun$withThreadLocalCaptured$1 at <unknown>:0) finished in 0.255 s
[2025-10-31T14:54:04.359+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-10-31T14:54:04.359+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
[2025-10-31T14:54:04.360+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO DAGScheduler: Job 9 finished: $anonfun$withThreadLocalCaptured$1 at <unknown>:0, took 0.261113 s
[2025-10-31T14:54:04.372+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 186.5 KiB, free 433.5 MiB)
[2025-10-31T14:54:04.373+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 68d0d8c522fe:34675 (size: 186.5 KiB, free: 433.9 MiB)
[2025-10-31T14:54:04.374+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO SparkContext: Created broadcast 22 from $anonfun$withThreadLocalCaptured$1 at <unknown>:0
[2025-10-31T14:54:04.380+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO FileSourceStrategy: Pushed Filters:
[2025-10-31T14:54:04.383+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO FileSourceStrategy: Post-Scan Filters:
[2025-10-31T14:54:04.422+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO CodeGenerator: Code generated in 18.747589 ms
[2025-10-31T14:54:04.426+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 203.1 KiB, free 433.3 MiB)
[2025-10-31T14:54:04.436+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 433.3 MiB)
[2025-10-31T14:54:04.437+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 68d0d8c522fe:34675 (size: 35.1 KiB, free: 433.8 MiB)
[2025-10-31T14:54:04.438+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO SparkContext: Created broadcast 23 from append at <unknown>:0
[2025-10-31T14:54:04.440+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-10-31T14:54:04.452+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 32.0 KiB, free 433.3 MiB)
[2025-10-31T14:54:04.485+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 29.7 KiB, free 433.3 MiB)
[2025-10-31T14:54:04.486+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 68d0d8c522fe:34675 (size: 29.7 KiB, free: 433.8 MiB)
[2025-10-31T14:54:04.487+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 68d0d8c522fe:34675 in memory (size: 5.5 KiB, free: 433.8 MiB)
[2025-10-31T14:54:04.488+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO SparkContext: Created broadcast 24 from broadcast at SparkWrite.java:193
[2025-10-31T14:54:04.490+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.30.0.7:35695 in memory (size: 5.5 KiB, free: 434.1 MiB)
[2025-10-31T14:54:04.491+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=hive_catalog.bronze.it_jobs_raw, format=PARQUET). The input RDD has 1 partitions.
[2025-10-31T14:54:04.500+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO SparkContext: Starting job: append at <unknown>:0
[2025-10-31T14:54:04.501+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO DAGScheduler: Got job 10 (append at <unknown>:0) with 1 output partitions
[2025-10-31T14:54:04.506+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO DAGScheduler: Final stage: ResultStage 13 (append at <unknown>:0)
[2025-10-31T14:54:04.508+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO DAGScheduler: Parents of final stage: List()
[2025-10-31T14:54:04.519+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO DAGScheduler: Missing parents: List()
[2025-10-31T14:54:04.532+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[39] at append at <unknown>:0), which has no missing parents
[2025-10-31T14:54:04.536+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 22.1 KiB, free 433.2 MiB)
[2025-10-31T14:54:04.539+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 9.8 KiB, free 433.2 MiB)
[2025-10-31T14:54:04.541+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 68d0d8c522fe:34675 (size: 9.8 KiB, free: 433.8 MiB)
[2025-10-31T14:54:04.547+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1585
[2025-10-31T14:54:04.555+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[39] at append at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-10-31T14:54:04.557+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2025-10-31T14:54:04.582+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (172.30.0.7, executor 0, partition 0, ANY, 9955 bytes)
[2025-10-31T14:54:04.619+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 172.30.0.7:35695 (size: 9.8 KiB, free: 434.1 MiB)
[2025-10-31T14:54:04.684+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 172.30.0.7:35695 (size: 186.5 KiB, free: 433.9 MiB)
[2025-10-31T14:54:04.712+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.30.0.7:35695 (size: 29.7 KiB, free: 433.9 MiB)
[2025-10-31T14:54:04.896+0000] {subprocess.py:106} INFO - 25/10/31 14:54:04 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 172.30.0.7:35695 (size: 35.1 KiB, free: 433.9 MiB)
[2025-10-31T14:54:05.186+0000] {subprocess.py:106} INFO - 25/10/31 14:54:05 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 604 ms on 172.30.0.7 (executor 0) (1/1)
[2025-10-31T14:54:05.190+0000] {subprocess.py:106} INFO - 25/10/31 14:54:05 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2025-10-31T14:54:05.191+0000] {subprocess.py:106} INFO - 25/10/31 14:54:05 INFO DAGScheduler: ResultStage 13 (append at <unknown>:0) finished in 0.658 s
[2025-10-31T14:54:05.191+0000] {subprocess.py:106} INFO - 25/10/31 14:54:05 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-10-31T14:54:05.192+0000] {subprocess.py:106} INFO - 25/10/31 14:54:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
[2025-10-31T14:54:05.192+0000] {subprocess.py:106} INFO - 25/10/31 14:54:05 INFO DAGScheduler: Job 10 finished: append at <unknown>:0, took 0.687878 s
[2025-10-31T14:54:05.193+0000] {subprocess.py:106} INFO - 25/10/31 14:54:05 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=hive_catalog.bronze.it_jobs_raw, format=PARQUET) is committing.
[2025-10-31T14:54:05.210+0000] {subprocess.py:106} INFO - 25/10/31 14:54:05 INFO SparkWrite: Committing append with 1 new data files to table hive_catalog.bronze.it_jobs_raw
[2025-10-31T14:54:05.881+0000] {subprocess.py:106} INFO - 25/10/31 14:54:05 INFO HiveTableOperations: Committed to table hive_catalog.bronze.it_jobs_raw with the new metadata location hdfs://dinhhoa-master:9000/user/ndh/warehouse/bronze.db/it_jobs_raw/metadata/00001-bad361a0-a319-4cf0-b3ba-bd4c7e3fddfa.metadata.json
[2025-10-31T14:54:05.882+0000] {subprocess.py:106} INFO - 25/10/31 14:54:05 INFO BaseMetastoreTableOperations: Successfully committed to table hive_catalog.bronze.it_jobs_raw in 370 ms
[2025-10-31T14:54:05.882+0000] {subprocess.py:106} INFO - 25/10/31 14:54:05 INFO SnapshotProducer: Committed snapshot 4312326354185098553 (MergeAppend)
[2025-10-31T14:54:05.890+0000] {subprocess.py:106} INFO - 25/10/31 14:54:05 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: hdfs://dinhhoa-master:9000/user/ndh/warehouse/bronze.db/it_jobs_raw/metadata/00001-bad361a0-a319-4cf0-b3ba-bd4c7e3fddfa.metadata.json
[2025-10-31T14:54:05.944+0000] {subprocess.py:106} INFO - 25/10/31 14:54:05 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=hive_catalog.bronze.it_jobs_raw, snapshotId=4312326354185098553, sequenceNumber=2, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.71599064S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=2}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=299}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=3433}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=26440}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=148983}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.3, app-id=app-20251031145347-0006, engine-name=spark, iceberg-version=Apache Iceberg 1.6.1 (commit 8e9d59d299be42b0bca9461457cd1e95dbaad086)}}
[2025-10-31T14:54:05.945+0000] {subprocess.py:106} INFO - 25/10/31 14:54:05 INFO SparkWrite: Committed in 733 ms
[2025-10-31T14:54:05.946+0000] {subprocess.py:106} INFO - 25/10/31 14:54:05 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=hive_catalog.bronze.it_jobs_raw, format=PARQUET) committed.
[2025-10-31T14:54:05.948+0000] {subprocess.py:106} INFO -  Loaded into: hive_catalog.bronze.it_jobs_raw
[2025-10-31T14:54:05.951+0000] {subprocess.py:106} INFO - 25/10/31 14:54:05 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-10-31T14:54:05.962+0000] {subprocess.py:106} INFO - 25/10/31 14:54:05 INFO SparkUI: Stopped Spark web UI at http://68d0d8c522fe:4040
[2025-10-31T14:54:05.969+0000] {subprocess.py:106} INFO - 25/10/31 14:54:05 INFO StandaloneSchedulerBackend: Shutting down all executors
[2025-10-31T14:54:05.970+0000] {subprocess.py:106} INFO - 25/10/31 14:54:05 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
[2025-10-31T14:54:05.998+0000] {subprocess.py:106} INFO - 25/10/31 14:54:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-10-31T14:54:06.023+0000] {subprocess.py:106} INFO - 25/10/31 14:54:06 INFO MemoryStore: MemoryStore cleared
[2025-10-31T14:54:06.025+0000] {subprocess.py:106} INFO - 25/10/31 14:54:06 INFO BlockManager: BlockManager stopped
[2025-10-31T14:54:06.027+0000] {subprocess.py:106} INFO - 25/10/31 14:54:06 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-10-31T14:54:06.033+0000] {subprocess.py:106} INFO - 25/10/31 14:54:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-10-31T14:54:06.065+0000] {subprocess.py:106} INFO - 25/10/31 14:54:06 INFO SparkContext: Successfully stopped SparkContext
[2025-10-31T14:54:06.255+0000] {subprocess.py:106} INFO - 25/10/31 14:54:06 INFO ShutdownHookManager: Shutdown hook called
[2025-10-31T14:54:06.256+0000] {subprocess.py:106} INFO - 25/10/31 14:54:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-72b76eea-8468-4a14-9e4b-30cc9f4869fe
[2025-10-31T14:54:06.259+0000] {subprocess.py:106} INFO - 25/10/31 14:54:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-c1db61f7-fd68-41f4-bfb5-55cc0045976d
[2025-10-31T14:54:06.261+0000] {subprocess.py:106} INFO - 25/10/31 14:54:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-c1db61f7-fd68-41f4-bfb5-55cc0045976d/pyspark-70916924-e99b-4472-b633-d81c11bbcdfd
[2025-10-31T14:54:06.328+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-10-31T14:54:06.410+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-10-31T14:54:06.411+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=etl_without_crawl, task_id=load_bronze, run_id=manual__2025-10-31T14:53:32.974462+00:00, execution_date=20251031T145332, start_date=20251031T145340, end_date=20251031T145406
[2025-10-31T14:54:06.470+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-10-31T14:54:06.505+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-10-31T14:54:06.507+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
