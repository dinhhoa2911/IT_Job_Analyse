[2025-10-30T09:58:53.576+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-10-30T09:58:53.611+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_without_crawl.load_bronze manual__2025-10-30T09:58:45.609923+00:00 [queued]>
[2025-10-30T09:58:53.621+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_without_crawl.load_bronze manual__2025-10-30T09:58:45.609923+00:00 [queued]>
[2025-10-30T09:58:53.621+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-10-30T09:58:53.632+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): load_bronze> on 2025-10-30 09:58:45.609923+00:00
[2025-10-30T09:58:53.636+0000] {standard_task_runner.py:72} INFO - Started process 68 to run task
[2025-10-30T09:58:53.640+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'etl_without_crawl', 'load_bronze', 'manual__2025-10-30T09:58:45.609923+00:00', '--job-id', '60', '--raw', '--subdir', 'DAGS_FOLDER/ETL_Without_Crawl.py', '--cfg-path', '/tmp/tmp4jpv0atl']
[2025-10-30T09:58:53.643+0000] {standard_task_runner.py:105} INFO - Job 60: Subtask load_bronze
[2025-10-30T09:58:53.667+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.8/site-packages/***/settings.py:209: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  SQL_ALCHEMY_CONN = conf.get("database", "SQL_ALCHEMY_CONN")

[2025-10-30T09:58:53.737+0000] {task_command.py:467} INFO - Running <TaskInstance: etl_without_crawl.load_bronze manual__2025-10-30T09:58:45.609923+00:00 [running]> on host a1de7f96d4f1
[2025-10-30T09:58:53.874+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='alert@datateam.local' AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='etl_without_crawl' AIRFLOW_CTX_TASK_ID='load_bronze' AIRFLOW_CTX_EXECUTION_DATE='2025-10-30T09:58:45.609923+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-10-30T09:58:45.609923+00:00'
[2025-10-30T09:58:53.875+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-10-30T09:58:53.890+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-10-30T09:58:53.892+0000] {subprocess.py:88} INFO - Running command: ['/bin/bash', '-c', "\n        docker exec spark-submit bash -c '\n        /opt/spark/bin/spark-submit             --master spark://spark-master:7077             --deploy-mode client             --name Load_Bronze_Hive_Iceberg             --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2             /opt/spark/scripts/Load_Bronze.py\n        '\n        "]
[2025-10-30T09:58:53.905+0000] {subprocess.py:99} INFO - Output:
[2025-10-30T09:58:56.628+0000] {subprocess.py:106} INFO - :: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-10-30T09:58:56.791+0000] {subprocess.py:106} INFO - Ivy Default Cache set to: /home/spark/.ivy2/cache
[2025-10-30T09:58:56.791+0000] {subprocess.py:106} INFO - The jars for the packages stored in: /home/spark/.ivy2/jars
[2025-10-30T09:58:56.799+0000] {subprocess.py:106} INFO - org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency
[2025-10-30T09:58:56.800+0000] {subprocess.py:106} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-74fc1c8b-8efd-479e-b253-da4f44c1f07e;1.0
[2025-10-30T09:58:56.800+0000] {subprocess.py:106} INFO - 	confs: [default]
[2025-10-30T09:58:57.772+0000] {subprocess.py:106} INFO - 	found org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 in central
[2025-10-30T09:58:58.026+0000] {subprocess.py:106} INFO - downloading https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.5.2/iceberg-spark-runtime-3.5_2.12-1.5.2.jar ...
[2025-10-30T09:58:59.986+0000] {subprocess.py:106} INFO - 	[SUCCESSFUL ] org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2!iceberg-spark-runtime-3.5_2.12.jar (2194ms)
[2025-10-30T09:58:59.990+0000] {subprocess.py:106} INFO - :: resolution report :: resolve 987ms :: artifacts dl 2199ms
[2025-10-30T09:58:59.991+0000] {subprocess.py:106} INFO - 	:: modules in use:
[2025-10-30T09:58:59.992+0000] {subprocess.py:106} INFO - 	org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 from central in [default]
[2025-10-30T09:58:59.993+0000] {subprocess.py:106} INFO - 	---------------------------------------------------------------------
[2025-10-30T09:58:59.993+0000] {subprocess.py:106} INFO - 	|                  |            modules            ||   artifacts   |
[2025-10-30T09:58:59.994+0000] {subprocess.py:106} INFO - 	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-10-30T09:58:59.994+0000] {subprocess.py:106} INFO - 	---------------------------------------------------------------------
[2025-10-30T09:58:59.995+0000] {subprocess.py:106} INFO - 	|      default     |   1   |   1   |   1   |   0   ||   1   |   1   |
[2025-10-30T09:58:59.996+0000] {subprocess.py:106} INFO - 	---------------------------------------------------------------------
[2025-10-30T09:58:59.996+0000] {subprocess.py:106} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-74fc1c8b-8efd-479e-b253-da4f44c1f07e
[2025-10-30T09:58:59.997+0000] {subprocess.py:106} INFO - 	confs: [default]
[2025-10-30T09:59:00.219+0000] {subprocess.py:106} INFO - 	1 artifacts copied, 0 already retrieved (40629kB/224ms)
[2025-10-30T09:59:00.477+0000] {subprocess.py:106} INFO - 25/10/30 09:59:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-10-30T09:59:02.115+0000] {subprocess.py:106} INFO - 25/10/30 09:59:02 INFO SparkContext: Running Spark version 3.5.3
[2025-10-30T09:59:02.115+0000] {subprocess.py:106} INFO - 25/10/30 09:59:02 INFO SparkContext: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64
[2025-10-30T09:59:02.116+0000] {subprocess.py:106} INFO - 25/10/30 09:59:02 INFO SparkContext: Java version 11.0.24
[2025-10-30T09:59:02.185+0000] {subprocess.py:106} INFO - 25/10/30 09:59:02 INFO ResourceUtils: ==============================================================
[2025-10-30T09:59:02.185+0000] {subprocess.py:106} INFO - 25/10/30 09:59:02 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-10-30T09:59:02.186+0000] {subprocess.py:106} INFO - 25/10/30 09:59:02 INFO ResourceUtils: ==============================================================
[2025-10-30T09:59:02.186+0000] {subprocess.py:106} INFO - 25/10/30 09:59:02 INFO SparkContext: Submitted application: Load_Bronze_Hive_Iceberg
[2025-10-30T09:59:02.246+0000] {subprocess.py:106} INFO - 25/10/30 09:59:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-10-30T09:59:02.256+0000] {subprocess.py:106} INFO - 25/10/30 09:59:02 INFO ResourceProfile: Limiting resource is cpu
[2025-10-30T09:59:02.257+0000] {subprocess.py:106} INFO - 25/10/30 09:59:02 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-10-30T09:59:02.497+0000] {subprocess.py:106} INFO - 25/10/30 09:59:02 INFO SecurityManager: Changing view acls to: spark
[2025-10-30T09:59:02.498+0000] {subprocess.py:106} INFO - 25/10/30 09:59:02 INFO SecurityManager: Changing modify acls to: spark
[2025-10-30T09:59:02.499+0000] {subprocess.py:106} INFO - 25/10/30 09:59:02 INFO SecurityManager: Changing view acls groups to:
[2025-10-30T09:59:02.499+0000] {subprocess.py:106} INFO - 25/10/30 09:59:02 INFO SecurityManager: Changing modify acls groups to:
[2025-10-30T09:59:02.500+0000] {subprocess.py:106} INFO - 25/10/30 09:59:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
[2025-10-30T09:59:02.917+0000] {subprocess.py:106} INFO - 25/10/30 09:59:02 INFO Utils: Successfully started service 'sparkDriver' on port 44219.
[2025-10-30T09:59:03.101+0000] {subprocess.py:106} INFO - 25/10/30 09:59:03 INFO SparkEnv: Registering MapOutputTracker
[2025-10-30T09:59:03.283+0000] {subprocess.py:106} INFO - 25/10/30 09:59:03 INFO SparkEnv: Registering BlockManagerMaster
[2025-10-30T09:59:03.314+0000] {subprocess.py:106} INFO - 25/10/30 09:59:03 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-10-30T09:59:03.315+0000] {subprocess.py:106} INFO - 25/10/30 09:59:03 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-10-30T09:59:03.323+0000] {subprocess.py:106} INFO - 25/10/30 09:59:03 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-10-30T09:59:03.376+0000] {subprocess.py:106} INFO - 25/10/30 09:59:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-39fb6c68-7501-4ccd-8a2f-bcb236fd97cc
[2025-10-30T09:59:03.397+0000] {subprocess.py:106} INFO - 25/10/30 09:59:03 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-10-30T09:59:03.427+0000] {subprocess.py:106} INFO - 25/10/30 09:59:03 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-10-30T09:59:03.624+0000] {subprocess.py:106} INFO - 25/10/30 09:59:03 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-10-30T09:59:03.698+0000] {subprocess.py:106} INFO - 25/10/30 09:59:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-10-30T09:59:03.743+0000] {subprocess.py:106} INFO - 25/10/30 09:59:03 INFO SparkContext: Added JAR file:///home/spark/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar at spark://68d0d8c522fe:44219/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar with timestamp 1761818342097
[2025-10-30T09:59:03.747+0000] {subprocess.py:106} INFO - 25/10/30 09:59:03 INFO SparkContext: Added file file:///home/spark/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar at spark://68d0d8c522fe:44219/files/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar with timestamp 1761818342097
[2025-10-30T09:59:03.749+0000] {subprocess.py:106} INFO - 25/10/30 09:59:03 INFO Utils: Copying /home/spark/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar to /tmp/spark-203b7e44-2f18-418f-a1a2-3b926e5de2b1/userFiles-b69dd189-71ec-487c-942f-0038a6000a25/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.5.2.jar
[2025-10-30T09:59:04.158+0000] {subprocess.py:106} INFO - 25/10/30 09:59:04 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2025-10-30T09:59:04.243+0000] {subprocess.py:106} INFO - 25/10/30 09:59:04 INFO TransportClientFactory: Successfully created connection to spark-master/172.30.0.6:7077 after 43 ms (0 ms spent in bootstraps)
[2025-10-30T09:59:04.796+0000] {subprocess.py:106} INFO - 25/10/30 09:59:04 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20251030095904-0000
[2025-10-30T09:59:04.827+0000] {subprocess.py:106} INFO - 25/10/30 09:59:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37329.
[2025-10-30T09:59:04.828+0000] {subprocess.py:106} INFO - 25/10/30 09:59:04 INFO NettyBlockTransferService: Server created on 68d0d8c522fe:37329
[2025-10-30T09:59:04.831+0000] {subprocess.py:106} INFO - 25/10/30 09:59:04 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-10-30T09:59:04.841+0000] {subprocess.py:106} INFO - 25/10/30 09:59:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 68d0d8c522fe, 37329, None)
[2025-10-30T09:59:04.845+0000] {subprocess.py:106} INFO - 25/10/30 09:59:04 INFO BlockManagerMasterEndpoint: Registering block manager 68d0d8c522fe:37329 with 434.4 MiB RAM, BlockManagerId(driver, 68d0d8c522fe, 37329, None)
[2025-10-30T09:59:04.848+0000] {subprocess.py:106} INFO - 25/10/30 09:59:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 68d0d8c522fe, 37329, None)
[2025-10-30T09:59:04.849+0000] {subprocess.py:106} INFO - 25/10/30 09:59:04 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20251030095904-0000/0 on worker-20251030074514-172.30.0.7-39239 (172.30.0.7:39239) with 1 core(s)
[2025-10-30T09:59:04.854+0000] {subprocess.py:106} INFO - 25/10/30 09:59:04 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 68d0d8c522fe, 37329, None)
[2025-10-30T09:59:04.855+0000] {subprocess.py:106} INFO - 25/10/30 09:59:04 INFO StandaloneSchedulerBackend: Granted executor ID app-20251030095904-0000/0 on hostPort 172.30.0.7:39239 with 1 core(s), 1024.0 MiB RAM
[2025-10-30T09:59:05.177+0000] {subprocess.py:106} INFO - 25/10/30 09:59:05 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2025-10-30T09:59:05.387+0000] {subprocess.py:106} INFO - 25/10/30 09:59:05 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20251030095904-0000/0 is now RUNNING
[2025-10-30T09:59:05.633+0000] {subprocess.py:106} INFO - 25/10/30 09:59:05 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-10-30T09:59:05.638+0000] {subprocess.py:106} INFO - 25/10/30 09:59:05 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
[2025-10-30T09:59:09.305+0000] {subprocess.py:106} INFO - 25/10/30 09:59:09 INFO InMemoryFileIndex: It took 125 ms to list leaf files for 1 paths.
[2025-10-30T09:59:09.496+0000] {subprocess.py:106} INFO - 25/10/30 09:59:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 203.7 KiB, free 434.2 MiB)
[2025-10-30T09:59:09.620+0000] {subprocess.py:106} INFO - 25/10/30 09:59:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.3 KiB, free 434.2 MiB)
[2025-10-30T09:59:09.626+0000] {subprocess.py:106} INFO - 25/10/30 09:59:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 68d0d8c522fe:37329 (size: 35.3 KiB, free: 434.4 MiB)
[2025-10-30T09:59:09.634+0000] {subprocess.py:106} INFO - 25/10/30 09:59:09 INFO SparkContext: Created broadcast 0 from csv at <unknown>:0
[2025-10-30T09:59:09.645+0000] {subprocess.py:106} INFO - 25/10/30 09:59:09 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.30.0.7:45290) with ID 0,  ResourceProfileId 0
[2025-10-30T09:59:09.746+0000] {subprocess.py:106} INFO - 25/10/30 09:59:09 INFO BlockManagerMasterEndpoint: Registering block manager 172.30.0.7:35397 with 434.4 MiB RAM, BlockManagerId(0, 172.30.0.7, 35397, None)
[2025-10-30T09:59:10.261+0000] {subprocess.py:106} INFO - 25/10/30 09:59:10 INFO FileInputFormat: Total input files to process : 1
[2025-10-30T09:59:10.266+0000] {subprocess.py:106} INFO - 25/10/30 09:59:10 INFO FileInputFormat: Total input files to process : 1
[2025-10-30T09:59:10.361+0000] {subprocess.py:106} INFO - 25/10/30 09:59:10 INFO SparkContext: Starting job: csv at <unknown>:0
[2025-10-30T09:59:10.391+0000] {subprocess.py:106} INFO - 25/10/30 09:59:10 INFO DAGScheduler: Got job 0 (csv at <unknown>:0) with 1 output partitions
[2025-10-30T09:59:10.392+0000] {subprocess.py:106} INFO - 25/10/30 09:59:10 INFO DAGScheduler: Final stage: ResultStage 0 (csv at <unknown>:0)
[2025-10-30T09:59:10.392+0000] {subprocess.py:106} INFO - 25/10/30 09:59:10 INFO DAGScheduler: Parents of final stage: List()
[2025-10-30T09:59:10.394+0000] {subprocess.py:106} INFO - 25/10/30 09:59:10 INFO DAGScheduler: Missing parents: List()
[2025-10-30T09:59:10.400+0000] {subprocess.py:106} INFO - 25/10/30 09:59:10 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at <unknown>:0), which has no missing parents
[2025-10-30T09:59:10.428+0000] {subprocess.py:106} INFO - 25/10/30 09:59:10 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.6 KiB, free 434.2 MiB)
[2025-10-30T09:59:10.457+0000] {subprocess.py:106} INFO - 25/10/30 09:59:10 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.4 KiB, free 434.2 MiB)
[2025-10-30T09:59:10.458+0000] {subprocess.py:106} INFO - 25/10/30 09:59:10 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 68d0d8c522fe:37329 (size: 4.4 KiB, free: 434.4 MiB)
[2025-10-30T09:59:10.459+0000] {subprocess.py:106} INFO - 25/10/30 09:59:10 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
[2025-10-30T09:59:10.481+0000] {subprocess.py:106} INFO - 25/10/30 09:59:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-10-30T09:59:10.482+0000] {subprocess.py:106} INFO - 25/10/30 09:59:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-10-30T09:59:11.074+0000] {subprocess.py:106} INFO - 25/10/30 09:59:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.30.0.7, executor 0, partition 0, PROCESS_LOCAL, 9484 bytes)
[2025-10-30T09:59:11.450+0000] {subprocess.py:106} INFO - 25/10/30 09:59:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.30.0.7:35397 (size: 4.4 KiB, free: 434.4 MiB)
[2025-10-30T09:59:11.792+0000] {subprocess.py:106} INFO - 25/10/30 09:59:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.30.0.7:35397 (size: 35.3 KiB, free: 434.4 MiB)
[2025-10-30T09:59:14.245+0000] {subprocess.py:106} INFO - 25/10/30 09:59:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3190 ms on 172.30.0.7 (executor 0) (1/1)
[2025-10-30T09:59:14.248+0000] {subprocess.py:106} INFO - 25/10/30 09:59:14 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-10-30T09:59:14.256+0000] {subprocess.py:106} INFO - 25/10/30 09:59:14 INFO DAGScheduler: ResultStage 0 (csv at <unknown>:0) finished in 3.834 s
[2025-10-30T09:59:14.263+0000] {subprocess.py:106} INFO - 25/10/30 09:59:14 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-10-30T09:59:14.265+0000] {subprocess.py:106} INFO - 25/10/30 09:59:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-10-30T09:59:14.267+0000] {subprocess.py:106} INFO - 25/10/30 09:59:14 INFO DAGScheduler: Job 0 finished: csv at <unknown>:0, took 2.478438 s
[2025-10-30T09:59:14.415+0000] {subprocess.py:106} INFO - 25/10/30 09:59:14 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 68d0d8c522fe:37329 in memory (size: 35.3 KiB, free: 434.4 MiB)
[2025-10-30T09:59:14.438+0000] {subprocess.py:106} INFO - 25/10/30 09:59:14 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.30.0.7:35397 in memory (size: 35.3 KiB, free: 434.4 MiB)
[2025-10-30T09:59:14.460+0000] {subprocess.py:106} INFO - 25/10/30 09:59:14 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 68d0d8c522fe:37329 in memory (size: 4.4 KiB, free: 434.4 MiB)
[2025-10-30T09:59:14.469+0000] {subprocess.py:106} INFO - 25/10/30 09:59:14 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.30.0.7:35397 in memory (size: 4.4 KiB, free: 434.4 MiB)
[2025-10-30T09:59:16.275+0000] {subprocess.py:106} INFO - 25/10/30 09:59:16 INFO FileSourceStrategy: Pushed Filters:
[2025-10-30T09:59:16.276+0000] {subprocess.py:106} INFO - 25/10/30 09:59:16 INFO FileSourceStrategy: Post-Scan Filters:
[2025-10-30T09:59:16.725+0000] {subprocess.py:106} INFO - 25/10/30 09:59:16 INFO CodeGenerator: Code generated in 218.806189 ms
[2025-10-30T09:59:16.734+0000] {subprocess.py:106} INFO - 25/10/30 09:59:16 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 203.1 KiB, free 434.2 MiB)
[2025-10-30T09:59:16.743+0000] {subprocess.py:106} INFO - 25/10/30 09:59:16 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 434.2 MiB)
[2025-10-30T09:59:16.744+0000] {subprocess.py:106} INFO - 25/10/30 09:59:16 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 68d0d8c522fe:37329 (size: 35.1 KiB, free: 434.4 MiB)
[2025-10-30T09:59:16.745+0000] {subprocess.py:106} INFO - 25/10/30 09:59:16 INFO SparkContext: Created broadcast 2 from count at <unknown>:0
[2025-10-30T09:59:16.760+0000] {subprocess.py:106} INFO - 25/10/30 09:59:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-10-30T09:59:16.826+0000] {subprocess.py:106} INFO - 25/10/30 09:59:16 INFO DAGScheduler: Registering RDD 7 (count at <unknown>:0) as input to shuffle 0
[2025-10-30T09:59:16.834+0000] {subprocess.py:106} INFO - 25/10/30 09:59:16 INFO DAGScheduler: Got map stage job 1 (count at <unknown>:0) with 1 output partitions
[2025-10-30T09:59:16.835+0000] {subprocess.py:106} INFO - 25/10/30 09:59:16 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (count at <unknown>:0)
[2025-10-30T09:59:16.835+0000] {subprocess.py:106} INFO - 25/10/30 09:59:16 INFO DAGScheduler: Parents of final stage: List()
[2025-10-30T09:59:16.839+0000] {subprocess.py:106} INFO - 25/10/30 09:59:16 INFO DAGScheduler: Missing parents: List()
[2025-10-30T09:59:16.842+0000] {subprocess.py:106} INFO - 25/10/30 09:59:16 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0), which has no missing parents
[2025-10-30T09:59:16.911+0000] {subprocess.py:106} INFO - 25/10/30 09:59:16 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 18.0 KiB, free 434.1 MiB)
[2025-10-30T09:59:16.924+0000] {subprocess.py:106} INFO - 25/10/30 09:59:16 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 434.1 MiB)
[2025-10-30T09:59:16.953+0000] {subprocess.py:106} INFO - 25/10/30 09:59:16 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 68d0d8c522fe:37329 (size: 8.9 KiB, free: 434.4 MiB)
[2025-10-30T09:59:16.957+0000] {subprocess.py:106} INFO - 25/10/30 09:59:16 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
[2025-10-30T09:59:16.960+0000] {subprocess.py:106} INFO - 25/10/30 09:59:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-10-30T09:59:16.963+0000] {subprocess.py:106} INFO - 25/10/30 09:59:16 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2025-10-30T09:59:16.973+0000] {subprocess.py:106} INFO - 25/10/30 09:59:16 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.30.0.7, executor 0, partition 0, ANY, 9944 bytes)
[2025-10-30T09:59:17.032+0000] {subprocess.py:106} INFO - 25/10/30 09:59:17 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.30.0.7:35397 (size: 8.9 KiB, free: 434.4 MiB)
[2025-10-30T09:59:17.745+0000] {subprocess.py:106} INFO - 25/10/30 09:59:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.30.0.7:35397 (size: 35.1 KiB, free: 434.4 MiB)
[2025-10-30T09:59:17.889+0000] {subprocess.py:106} INFO - 25/10/30 09:59:17 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 947 ms on 172.30.0.7 (executor 0) (1/1)
[2025-10-30T09:59:17.890+0000] {subprocess.py:106} INFO - 25/10/30 09:59:17 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-10-30T09:59:17.894+0000] {subprocess.py:106} INFO - 25/10/30 09:59:17 INFO DAGScheduler: ShuffleMapStage 1 (count at <unknown>:0) finished in 1.041 s
[2025-10-30T09:59:17.897+0000] {subprocess.py:106} INFO - 25/10/30 09:59:17 INFO DAGScheduler: looking for newly runnable stages
[2025-10-30T09:59:17.898+0000] {subprocess.py:106} INFO - 25/10/30 09:59:17 INFO DAGScheduler: running: Set()
[2025-10-30T09:59:17.899+0000] {subprocess.py:106} INFO - 25/10/30 09:59:17 INFO DAGScheduler: waiting: Set()
[2025-10-30T09:59:17.899+0000] {subprocess.py:106} INFO - 25/10/30 09:59:17 INFO DAGScheduler: failed: Set()
[2025-10-30T09:59:17.949+0000] {subprocess.py:106} INFO - 25/10/30 09:59:17 INFO CodeGenerator: Code generated in 14.488979 ms
[2025-10-30T09:59:17.979+0000] {subprocess.py:106} INFO - 25/10/30 09:59:17 INFO SparkContext: Starting job: count at <unknown>:0
[2025-10-30T09:59:17.981+0000] {subprocess.py:106} INFO - 25/10/30 09:59:17 INFO DAGScheduler: Got job 2 (count at <unknown>:0) with 1 output partitions
[2025-10-30T09:59:17.982+0000] {subprocess.py:106} INFO - 25/10/30 09:59:17 INFO DAGScheduler: Final stage: ResultStage 3 (count at <unknown>:0)
[2025-10-30T09:59:17.984+0000] {subprocess.py:106} INFO - 25/10/30 09:59:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
[2025-10-30T09:59:17.984+0000] {subprocess.py:106} INFO - 25/10/30 09:59:17 INFO DAGScheduler: Missing parents: List()
[2025-10-30T09:59:17.985+0000] {subprocess.py:106} INFO - 25/10/30 09:59:17 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0), which has no missing parents
[2025-10-30T09:59:17.991+0000] {subprocess.py:106} INFO - 25/10/30 09:59:17 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 12.5 KiB, free 434.1 MiB)
[2025-10-30T09:59:18.006+0000] {subprocess.py:106} INFO - 25/10/30 09:59:18 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.1 MiB)
[2025-10-30T09:59:18.007+0000] {subprocess.py:106} INFO - 25/10/30 09:59:18 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 68d0d8c522fe:37329 (size: 5.9 KiB, free: 434.4 MiB)
[2025-10-30T09:59:18.008+0000] {subprocess.py:106} INFO - 25/10/30 09:59:18 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
[2025-10-30T09:59:18.009+0000] {subprocess.py:106} INFO - 25/10/30 09:59:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-10-30T09:59:18.009+0000] {subprocess.py:106} INFO - 25/10/30 09:59:18 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2025-10-30T09:59:18.014+0000] {subprocess.py:106} INFO - 25/10/30 09:59:18 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (172.30.0.7, executor 0, partition 0, NODE_LOCAL, 9331 bytes)
[2025-10-30T09:59:18.037+0000] {subprocess.py:106} INFO - 25/10/30 09:59:18 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.30.0.7:35397 (size: 5.9 KiB, free: 434.4 MiB)
[2025-10-30T09:59:18.085+0000] {subprocess.py:106} INFO - 25/10/30 09:59:18 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.30.0.7:45290
[2025-10-30T09:59:18.245+0000] {subprocess.py:106} INFO - 25/10/30 09:59:18 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 232 ms on 172.30.0.7 (executor 0) (1/1)
[2025-10-30T09:59:18.246+0000] {subprocess.py:106} INFO - 25/10/30 09:59:18 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2025-10-30T09:59:18.248+0000] {subprocess.py:106} INFO - 25/10/30 09:59:18 INFO DAGScheduler: ResultStage 3 (count at <unknown>:0) finished in 0.258 s
[2025-10-30T09:59:18.249+0000] {subprocess.py:106} INFO - 25/10/30 09:59:18 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-10-30T09:59:18.249+0000] {subprocess.py:106} INFO - 25/10/30 09:59:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2025-10-30T09:59:18.250+0000] {subprocess.py:106} INFO - 25/10/30 09:59:18 INFO DAGScheduler: Job 2 finished: count at <unknown>:0, took 0.269013 s
[2025-10-30T09:59:18.256+0000] {subprocess.py:106} INFO -  Record loaded from CSV: 3134
[2025-10-30T09:59:18.258+0000] {subprocess.py:106} INFO - root
[2025-10-30T09:59:18.259+0000] {subprocess.py:106} INFO -  |-- job_title: string (nullable = true)
[2025-10-30T09:59:18.260+0000] {subprocess.py:106} INFO -  |-- company_name: string (nullable = true)
[2025-10-30T09:59:18.261+0000] {subprocess.py:106} INFO -  |-- location: string (nullable = true)
[2025-10-30T09:59:18.261+0000] {subprocess.py:106} INFO -  |-- skills_required: string (nullable = true)
[2025-10-30T09:59:18.261+0000] {subprocess.py:106} INFO -  |-- date_posted: string (nullable = true)
[2025-10-30T09:59:18.262+0000] {subprocess.py:106} INFO -  |-- job_link: string (nullable = true)
[2025-10-30T09:59:18.262+0000] {subprocess.py:106} INFO -  |-- job_category: string (nullable = true)
[2025-10-30T09:59:18.262+0000] {subprocess.py:106} INFO -  |-- work_mode: string (nullable = true)
[2025-10-30T09:59:18.263+0000] {subprocess.py:106} INFO -  |-- crawl_date: string (nullable = true)
[2025-10-30T09:59:18.263+0000] {subprocess.py:106} INFO -  |-- date_only: string (nullable = true)
[2025-10-30T09:59:18.263+0000] {subprocess.py:106} INFO - 
[2025-10-30T09:59:18.319+0000] {subprocess.py:106} INFO - 25/10/30 09:59:18 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 68d0d8c522fe:37329 in memory (size: 5.9 KiB, free: 434.4 MiB)
[2025-10-30T09:59:18.325+0000] {subprocess.py:106} INFO - 25/10/30 09:59:18 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.30.0.7:35397 in memory (size: 5.9 KiB, free: 434.4 MiB)
[2025-10-30T09:59:18.342+0000] {subprocess.py:106} INFO - 25/10/30 09:59:18 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 68d0d8c522fe:37329 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2025-10-30T09:59:18.345+0000] {subprocess.py:106} INFO - 25/10/30 09:59:18 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.30.0.7:35397 in memory (size: 8.9 KiB, free: 434.4 MiB)
[2025-10-30T09:59:18.730+0000] {subprocess.py:106} INFO - 25/10/30 09:59:18 INFO HiveConf: Found configuration file null
[2025-10-30T09:59:18.938+0000] {subprocess.py:106} INFO - 25/10/30 09:59:18 INFO metastore: Trying to connect to metastore with URI thrift://hive-metastore:9083
[2025-10-30T09:59:18.976+0000] {subprocess.py:106} INFO - 25/10/30 09:59:18 INFO metastore: Opened a connection to metastore, current connections: 1
[2025-10-30T09:59:19.038+0000] {subprocess.py:106} INFO - 25/10/30 09:59:19 INFO metastore: Connected to metastore.
[2025-10-30T09:59:19.755+0000] {subprocess.py:106} INFO - 25/10/30 09:59:19 INFO HiveCatalog: Created namespace: bronze
[2025-10-30T09:59:19.955+0000] {subprocess.py:106} INFO - 25/10/30 09:59:19 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 68d0d8c522fe:37329 in memory (size: 35.1 KiB, free: 434.4 MiB)
[2025-10-30T09:59:19.962+0000] {subprocess.py:106} INFO - 25/10/30 09:59:19 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.30.0.7:35397 in memory (size: 35.1 KiB, free: 434.4 MiB)
[2025-10-30T09:59:19.979+0000] {subprocess.py:106} INFO - 25/10/30 09:59:19 INFO BaseMetastoreCatalog: Table properties set at catalog level through catalog properties: {}
[2025-10-30T09:59:19.987+0000] {subprocess.py:106} INFO - 25/10/30 09:59:19 INFO BaseMetastoreCatalog: Table properties enforced at catalog level through catalog properties: {}
[2025-10-30T09:59:20.114+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO SparkWrite: Requesting 0 bytes advisory partition size for table bronze.it_jobs_raw
[2025-10-30T09:59:20.114+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table bronze.it_jobs_raw
[2025-10-30T09:59:20.117+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO SparkWrite: Requesting [] as write ordering for table bronze.it_jobs_raw
[2025-10-30T09:59:20.132+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO FileSourceStrategy: Pushed Filters:
[2025-10-30T09:59:20.133+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO FileSourceStrategy: Post-Scan Filters:
[2025-10-30T09:59:20.185+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO CodeGenerator: Code generated in 22.269235 ms
[2025-10-30T09:59:20.190+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 203.1 KiB, free 434.2 MiB)
[2025-10-30T09:59:20.202+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 434.2 MiB)
[2025-10-30T09:59:20.203+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 68d0d8c522fe:37329 (size: 35.1 KiB, free: 434.4 MiB)
[2025-10-30T09:59:20.207+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO SparkContext: Created broadcast 5 from create at <unknown>:0
[2025-10-30T09:59:20.210+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-10-30T09:59:20.454+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-10-30T09:59:20.466+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 29.4 KiB, free 434.1 MiB)
[2025-10-30T09:59:20.467+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 68d0d8c522fe:37329 (size: 29.4 KiB, free: 434.3 MiB)
[2025-10-30T09:59:20.468+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO SparkContext: Created broadcast 6 from broadcast at SparkWrite.java:193
[2025-10-30T09:59:20.470+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=bronze.it_jobs_raw, format=PARQUET). The input RDD has 1 partitions.
[2025-10-30T09:59:20.482+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO SparkContext: Starting job: create at <unknown>:0
[2025-10-30T09:59:20.483+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO DAGScheduler: Got job 3 (create at <unknown>:0) with 1 output partitions
[2025-10-30T09:59:20.484+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO DAGScheduler: Final stage: ResultStage 4 (create at <unknown>:0)
[2025-10-30T09:59:20.484+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO DAGScheduler: Parents of final stage: List()
[2025-10-30T09:59:20.485+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO DAGScheduler: Missing parents: List()
[2025-10-30T09:59:20.486+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[13] at create at <unknown>:0), which has no missing parents
[2025-10-30T09:59:20.489+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 20.8 KiB, free 434.1 MiB)
[2025-10-30T09:59:20.491+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 9.3 KiB, free 434.1 MiB)
[2025-10-30T09:59:20.493+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 68d0d8c522fe:37329 (size: 9.3 KiB, free: 434.3 MiB)
[2025-10-30T09:59:20.494+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
[2025-10-30T09:59:20.494+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at create at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-10-30T09:59:20.494+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2025-10-30T09:59:20.496+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (172.30.0.7, executor 0, partition 0, ANY, 9955 bytes)
[2025-10-30T09:59:20.514+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.30.0.7:35397 (size: 9.3 KiB, free: 434.4 MiB)
[2025-10-30T09:59:20.668+0000] {subprocess.py:106} INFO - 25/10/30 09:59:20 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.30.0.7:35397 (size: 29.4 KiB, free: 434.4 MiB)
[2025-10-30T09:59:21.622+0000] {subprocess.py:106} INFO - 25/10/30 09:59:21 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.30.0.7:35397 (size: 35.1 KiB, free: 434.3 MiB)
[2025-10-30T09:59:22.441+0000] {subprocess.py:106} INFO - 25/10/30 09:59:22 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 1944 ms on 172.30.0.7 (executor 0) (1/1)
[2025-10-30T09:59:22.449+0000] {subprocess.py:106} INFO - 25/10/30 09:59:22 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2025-10-30T09:59:22.450+0000] {subprocess.py:106} INFO - 25/10/30 09:59:22 INFO DAGScheduler: ResultStage 4 (create at <unknown>:0) finished in 1.954 s
[2025-10-30T09:59:22.451+0000] {subprocess.py:106} INFO - 25/10/30 09:59:22 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-10-30T09:59:22.452+0000] {subprocess.py:106} INFO - 25/10/30 09:59:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2025-10-30T09:59:22.452+0000] {subprocess.py:106} INFO - 25/10/30 09:59:22 INFO DAGScheduler: Job 3 finished: create at <unknown>:0, took 1.960508 s
[2025-10-30T09:59:22.453+0000] {subprocess.py:106} INFO - 25/10/30 09:59:22 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=bronze.it_jobs_raw, format=PARQUET) is committing.
[2025-10-30T09:59:22.476+0000] {subprocess.py:106} INFO - 25/10/30 09:59:22 INFO SparkWrite: Committing append with 1 new data files to table bronze.it_jobs_raw
[2025-10-30T09:59:22.977+0000] {subprocess.py:106} INFO - 25/10/30 09:59:22 INFO SnapshotProducer: Committed snapshot 2601246677096618756 (MergeAppend)
[2025-10-30T09:59:23.100+0000] {subprocess.py:106} INFO - 25/10/30 09:59:23 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=bronze.it_jobs_raw, snapshotId=2601246677096618756, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.601177123S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=1}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=3134}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=3134}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=122543}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=122543}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.3, app-id=app-20251030095904-0000, engine-name=spark, iceberg-version=Apache Iceberg 1.6.1 (commit 8e9d59d299be42b0bca9461457cd1e95dbaad086)}}
[2025-10-30T09:59:23.105+0000] {subprocess.py:106} INFO - 25/10/30 09:59:23 INFO SparkWrite: Committed in 622 ms
[2025-10-30T09:59:23.106+0000] {subprocess.py:106} INFO - 25/10/30 09:59:23 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=bronze.it_jobs_raw, format=PARQUET) committed.
[2025-10-30T09:59:23.654+0000] {subprocess.py:106} INFO - 25/10/30 09:59:23 INFO HiveTableOperations: Committed to table hive_catalog.bronze.it_jobs_raw with the new metadata location hdfs://dinhhoa-master:9000/user/ndh/warehouse/bronze.db/it_jobs_raw/metadata/00000-5f98341f-8c6b-4263-b5b9-f76ab8c7f9a5.metadata.json
[2025-10-30T09:59:23.655+0000] {subprocess.py:106} INFO - 25/10/30 09:59:23 INFO BaseMetastoreTableOperations: Successfully committed to table hive_catalog.bronze.it_jobs_raw in 549 ms
[2025-10-30T09:59:23.659+0000] {subprocess.py:106} INFO -  Loaded into: hive_catalog.bronze.it_jobs_raw
[2025-10-30T09:59:23.659+0000] {subprocess.py:106} INFO - 25/10/30 09:59:23 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-10-30T09:59:23.676+0000] {subprocess.py:106} INFO - 25/10/30 09:59:23 INFO SparkUI: Stopped Spark web UI at http://68d0d8c522fe:4040
[2025-10-30T09:59:23.682+0000] {subprocess.py:106} INFO - 25/10/30 09:59:23 INFO StandaloneSchedulerBackend: Shutting down all executors
[2025-10-30T09:59:23.683+0000] {subprocess.py:106} INFO - 25/10/30 09:59:23 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
[2025-10-30T09:59:23.707+0000] {subprocess.py:106} INFO - 25/10/30 09:59:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-10-30T09:59:23.729+0000] {subprocess.py:106} INFO - 25/10/30 09:59:23 INFO MemoryStore: MemoryStore cleared
[2025-10-30T09:59:23.730+0000] {subprocess.py:106} INFO - 25/10/30 09:59:23 INFO BlockManager: BlockManager stopped
[2025-10-30T09:59:23.748+0000] {subprocess.py:106} INFO - 25/10/30 09:59:23 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-10-30T09:59:23.753+0000] {subprocess.py:106} INFO - 25/10/30 09:59:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-10-30T09:59:23.766+0000] {subprocess.py:106} INFO - 25/10/30 09:59:23 INFO SparkContext: Successfully stopped SparkContext
[2025-10-30T09:59:24.203+0000] {subprocess.py:106} INFO - 25/10/30 09:59:24 INFO ShutdownHookManager: Shutdown hook called
[2025-10-30T09:59:24.204+0000] {subprocess.py:106} INFO - 25/10/30 09:59:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-4a00a1ba-1c44-47af-9021-48790b1c9b5c
[2025-10-30T09:59:24.207+0000] {subprocess.py:106} INFO - 25/10/30 09:59:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-203b7e44-2f18-418f-a1a2-3b926e5de2b1
[2025-10-30T09:59:24.210+0000] {subprocess.py:106} INFO - 25/10/30 09:59:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-203b7e44-2f18-418f-a1a2-3b926e5de2b1/pyspark-af7ef562-9a91-4d99-a40f-40bbd578afc0
[2025-10-30T09:59:24.310+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-10-30T09:59:24.382+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-10-30T09:59:24.383+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=etl_without_crawl, task_id=load_bronze, run_id=manual__2025-10-30T09:58:45.609923+00:00, execution_date=20251030T095845, start_date=20251030T095853, end_date=20251030T095924
[2025-10-30T09:59:24.501+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-10-30T09:59:24.585+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-10-30T09:59:24.588+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
